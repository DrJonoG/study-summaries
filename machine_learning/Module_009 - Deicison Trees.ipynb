{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94789ec-5081-4692-bde2-aa5869467b9c",
   "metadata": {},
   "source": [
    "# Module 9 - Decision Trees (Part One)\n",
    "\n",
    "---\n",
    "\n",
    "## Module Overview\n",
    "\n",
    "- Learn how to use decision trees to solve regression and classification problems\n",
    "- Module 9 looks at individual decision trees, Module 10 will cover combinations of trees and tree ensembles\n",
    "\n",
    "## Learning outcomes\n",
    "\n",
    "- LO1: Understand how a decision tree makes predictionsd\n",
    "- LO2: Learn the difference between conducting splits on categorical and numerical input varaibles\n",
    "- LO3: Measure purity in categorical models using entropy and gini index\n",
    "- LO4: Compute a decision tree in python\n",
    "- LO5: Compare pruning strategies\n",
    "- LO6: Recognise differences in constructing regression rather than classification trees\n",
    "- LO7: The application of decision trees within the context of solving a business challenge.\n",
    "- LO8: Control the bias-variance trade off via pruning\n",
    "\n",
    "## Misc and Keywords\n",
    "- **Divide and conquer principle** splits the complex task of growing an entire tree into more manageable tasks of branching individual nodes at a time\n",
    "- **Greedy principle** which chooses splits based on their immediate information gain and never revisits a decision once its been made\n",
    "- **Gini index** A measure of impurity used in classification trees; lower values indicate purer splits.\n",
    "- **Entrophy** A metric from information theory that quantifies the impurity of a dataset; high entropy means more disorder, while low entropy means more uniformity.\n",
    "- **Information gain** The reduction in entropy (or impurity) achieved by splitting a dataset on a particular feature; used to decide the best splits in decision trees.\n",
    "\n",
    "---\n",
    "\n",
    "## Formulas\n",
    "- Entropy\n",
    "$$H(N) = \\sum_{i=1}^{m}-p_{i}log_{2}(p_{i})$$\n",
    "- Gini Index\n",
    " $$G(N) = \\sum_{i=1}^{m}p_{i}(1-p_{i})$$\n",
    "- Information Gain\n",
    "$$\\text{IG} = f(N) - [w_{1}f(N_{1}) + ... + w_{n}f(N_{n})]$$\n",
    "\n",
    "---\n",
    "\n",
    "## Decision Tree Basics\n",
    "- How to choose the best branch:\n",
    "    - **Gini index**\n",
    "    - **Entropy**\n",
    "- How to stop overfitting:\n",
    "    - **Pre-pruning** stops a tree from growing when the improvement becomes too small\n",
    "    - **Post-pruning** cuts the tree down once it has been grown\n",
    "- Decision trees, in this context, are not used to make decisions but to classify new data points\n",
    "- Decision trees grow from bottom to top\n",
    "    - The first node is known as the root node\n",
    "    - Terminating nodes are known as leaf nodes\n",
    "    - Internal nodes are decision nodes\n",
    "- **Misclassification Rate** is a measure of how often the model incorrectly predicts the target\n",
    "    - $\\text{Misclassification Rate = }\\frac{\\text{Number of Incorrect Predictions}}{\\text{Total Number of Predictions}}$\n",
    "    - $\\text{Accuracy = 1 - Misclassification Rate}$\n",
    "- Decision trees are **interpretable** by humans\n",
    "- General process is:\n",
    "    - Start with empty decision tree (undivided)\n",
    "    - Choose 'optimal' predictor on which to split based on splitting criterion\n",
    "    - Repeat until stopping condition is met "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb10e0f4-2555-450a-9259-21a65d1cc84d",
   "metadata": {},
   "source": [
    "### How to split trees\n",
    "- Splitting categorical variables, three common methods:\n",
    "    - Create one child for each possible category\n",
    "        - These can introduce bias\n",
    "    - Binary splits\n",
    "        - Create a child for one particular catgeory, and everything else goes in the other node i.e. Left Node: Low, Right Node: Is not Low\n",
    "            - Can result in deep trees due to small decision making at each branch \n",
    "        - Create a child, left node, that is a subset of categories, and everything not in the subset goes in the right node\n",
    "            - Requires a large number of possible splits to be consider       \n",
    "- Splitting numerical variables\n",
    "    - Considers only binary splits\n",
    "    - Every split is considered that is half way between two consecutive values\n",
    "- How do we know if a split is good?\n",
    "    - Measure the **purity**\n",
    "        - Want to perform splits that maximise the purity of the tree\n",
    "        - The increase in purity is measured by information gain\n",
    "        - Purity can be measured using **entropy** or the **Gini Index**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be6e7d7-e5fd-4b93-9cd8-3e721c352a90",
   "metadata": {},
   "source": [
    "### Choosing the Best Split - Measuring Purity\n",
    "#### Two main methods:\n",
    "\n",
    "### 1. Entropy\n",
    "- When applying entropy to decision trees, we use the formula:\n",
    "\n",
    "  $$\n",
    "  H(N) = - \\sum_{i=1}^{m} p_i \\log_2(p_i)\n",
    "  $$\n",
    "\n",
    "  where $p_i$ is the fraction of training data belonging to category $C_i, \\, i = 1, \\dots, m$.\n",
    "\n",
    "- The **purity of child nodes** ($N_1, \\dots, N_n$) resulting from a split is given by $H(N_1), \\dots, H(N_n)$.\n",
    "- The **best split** is chosen based on **Information Gain**:\n",
    "\n",
    "  $$\n",
    "  IG = H(N) - \\left[ w_1 H(N_1) + w_2 H(N_2) + \\dots + w_n H(N_n) \\right]\n",
    "  $$\n",
    "\n",
    "  where $w_i$ is the proportion of data in node $N_i$ (e.g., 0.5 if the node contains 50% of the data).\n",
    "\n",
    "- **Steps to calculate entropy for a split:**\n",
    "  1. Compute the **entropy of the current node** before the split.\n",
    "  2. Compute the **entropy of each child node** after the split.\n",
    "  3. Compute the **Information Gain**.\n",
    "  4. Select the split that **maximises** Information Gain.\n",
    "\n",
    "- **Entropy ranges from 0 to $\\log_2(m)$,** where $m$ is the number of classes.\n",
    "- **A split is taken if it results in positive Information Gain** (i.e., reduces entropy).\n",
    "\n",
    "### 2. Gini Index\n",
    "- The **Gini index** measures the probability that two randomly chosen instances from the dataset **belong to different classes**.\n",
    "- It is given by:\n",
    "\n",
    "  $$\n",
    "  G(N) = \\sum_{i=1}^{m} p_i (1 - p_i) = 1 - \\sum_{i=1}^{m} p_i^2\n",
    "  $$\n",
    "\n",
    "  where $p_i$ is the fraction of training data belonging to category $C_i, \\, i = 1, \\dots, m$.\n",
    "\n",
    "- **Interpretation:**  \n",
    "  - If we **randomly pick** one training sample and **randomly label** it according to the class proportions, the Gini index **computes the probability of incorrect classification**.\n",
    "  - A **higher Gini index** indicates a **less pure node** (more disorder).\n",
    "  - A **lower Gini index** is **preferred** as it indicates **higher purity**.\n",
    "\n",
    "- **The Gini index ranges from 0 to $1 - \\frac{1}{m}$:**\n",
    "  - $G = 0$ when the node is **pure** (only one class is present).\n",
    "  - $G$ reaches its **maximum when classes are evenly distributed**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76a596f-105d-4f27-8105-4518dae66a51",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Entropy in Decision Trees\n",
    "\n",
    "In the context of **decision trees**, **entropy** is a measure of **impurity** or **uncertainty** in a dataset. It helps in deciding how to split the data to best classify the data points.\n",
    "\n",
    "The entropy formula is:\n",
    "\n",
    "$$\n",
    "H(X) = -\\sum_{i=1}^{m} p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $p_i$ is the probability of the $i$-th outcome of the random variable $X$.\n",
    "- $m$ is the total number of possible outcomes of $X$.\n",
    "\n",
    "#### Low Entropy (Pure Nodes)\n",
    "- **Low entropy** means the dataset is **pure** or **predictable**.\n",
    "- If most data points in a node belong to the **same class**, entropy is low.\n",
    "- **Example:** If **90% of the data** in a node belongs to **Class A** and only **10% belongs to Class B**, the entropy is **low** because the outcome is **predictable**.\n",
    "\n",
    "For binary classification, entropy is calculated as:\n",
    "\n",
    "$$\n",
    "H(X) = -p \\log_2(p) - (1 - p) \\log_2(1 - p)\n",
    "$$\n",
    "\n",
    "where $p$ is the proportion of one class. If $p = 1$ or $p = 0$ (i.e., only one class is present), then:\n",
    "\n",
    "$$\n",
    "H(X) = 0\n",
    "$$\n",
    "\n",
    "indicating a **perfectly pure node**.\n",
    "\n",
    "#### High Entropy (Impure Nodes)\n",
    "- **High entropy** means the dataset is **impure** or **uncertain**.\n",
    "- If a node contains a **mixed set of classes**, entropy is high.\n",
    "- **Example:** If a node has **50% Class A** and **50% Class B**, it has **maximum entropy**.\n",
    "\n",
    "For binary classification ($m = 2$), the **maximum entropy is 1**, occurring when $p = 0.5$:\n",
    "\n",
    "$$\n",
    "H(X) = -0.5 \\log_2(0.5) - 0.5 \\log_2(0.5) = 1\n",
    "$$\n",
    "\n",
    "For **multi-class classification** ($m > 2$), entropy can be **higher** and reaches a **maximum of $\\log_2(m)$** when all classes are equally likely.\n",
    "\n",
    "## Entropy in Decision Tree Splitting\n",
    "- Decision trees aim to **reduce entropy** at each split.\n",
    "- **Information Gain (IG)** measures the reduction in entropy after a split:\n",
    "\n",
    "$$\n",
    "IG = H(\\text{parent}) - \\sum_{i=1}^{n} w_i H(N_i)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $H(\\text{parent})$ is the entropy of the current node before splitting.\n",
    "- $H(N_i)$ is the entropy of child node $N_i$.\n",
    "- $w_i$ is the proportion of data in node $N_i$ (i.e., $w_i = \\frac{|N_i|}{|N_{\\text{parent}}|}$).\n",
    "\n",
    "- **Higher Information Gain** means a better split, leading to **purer child nodes**.\n",
    "- If the entropy remains high after a split, further splitting is needed.\n",
    "\n",
    "#### **Summary:**\n",
    "\n",
    "- **Low Entropy**: Data is mostly homogeneous, and a decision tree should stop splitting here because the classes are predictable.\n",
    "- **High Entropy**: Data is heterogeneous, and a decision tree should continue splitting to reduce uncertainty and increase purity in the resulting nodes.\n",
    "\n",
    "The goal in decision trees is to create splits that minimise entropy and maximise information gain to make the data in each subsequent node more pure, thereby improving the decision-making ability of the tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576f4596-744e-4c7e-bd8b-a462865125a6",
   "metadata": {},
   "source": [
    "### How to Construct a Tree\n",
    "- We construct classification trees by maximising the information gain which is based on either the gini index or entropy.\n",
    "- We evaluate the performance of a classification tree by looking at the error rate\n",
    "- Common question that arises is, why don't we grow trees by looking at the error rate directly from the beginning?\n",
    "    - The initial split wouldn't improve the error rate, and therefore wouldn't be taken\n",
    "    - The error rate can be too crude to function as a measure of progress in the tree construction process\n",
    "    - May say to stop the construction early, despite possibilites of additional gains\n",
    "    - In comparison to the entropy which is more sensitive and can detect even minor immediate progress.\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0416bd7-b20b-4646-89db-fd5bb2f8d75d",
   "metadata": {},
   "source": [
    "### How to Prune a Classification Tree\n",
    "- We can stop growing a classification tree when:\n",
    "    - There are no more features left to split on.\n",
    "    - Until all data is correctly classified.\n",
    "    - **however** both of these result in overfitting, and result a tree that is too large\n",
    "- Approaches to overcome this are:\n",
    "    - Decision tree pre-pruning\n",
    "        - Stop growing the tree when the information gain no longer exceeds a pre-specified threshold value\n",
    "        - However, this may result in prematurely executing the construction of the tree, a manifestation of the greedy principle.\n",
    "    - Decision tree post-pruning\n",
    "        - More favourable that pre-pruning as it doesn't prematurely end        \n",
    "#### Decision tree post-pruning\n",
    "- Also known as cost complexity pruning, the algorithm is below:\n",
    "    1. Split the available data into training data and validation data\n",
    "    2. Grow a complete tree $T_0$ from training data\n",
    "    3. For every value $\\alpha >= 0$ find subtree that minimises\n",
    "        $$[\\sum_{\\text{N leaf node}}\\text{weighted purity(N)}] + \\alpha \\cdot \\text{num of leaf nodes}$$\n",
    "        -  Where the first section is the impurity on training data, and the second part is a penalty for tree size\n",
    "        -  If alpha is small, we have a large tree. If alpha is large, we end up with just the root node.\n",
    "        -  For every value of alpha we have a different subtree\n",
    "    4. Choose the best subtree using the validation data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014c73ce-798f-46d0-b151-6a8da72082d3",
   "metadata": {},
   "source": [
    "## Regression Trees\n",
    "- Only two things need to change between a classification and regression tree, they are;\n",
    "    - how we predict the outcome\n",
    "        - Instead of using a majority vote, we use the mean value of the training data to predict the outcome\n",
    "        $$\\hat{Y} = \\frac{1}{n}\\sum_{i=1}^{n}Y_i$$ \n",
    "    - how do we choose the best split\n",
    "        - No longer use entropy or gini index, instead impurity is defined by RMSE\n",
    "        $$MSE(\\hat{Y}) = \\frac{1}{n}\\sum_{i=1}^{n}(Y_{i} - \\hat{Y})^{2}$$\n",
    "- Regression trees assume the outcome is a piecewise constant rectangular function of input variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e871df-ad8e-4114-9d72-136fd252eb55",
   "metadata": {},
   "source": [
    "## Creating Decision Trees on a Computer\n",
    "- Below is an example of a decision tree algorithm. We have chosen to terminate the branches when the information gain is less than 0.02. This is a slightly arbitrary choice that will be discussed further when you learn about pruning in the following sections.\n",
    "- Algorithm:\n",
    "    - Calculate the Gini index of the parent node.\n",
    "    - For each possible split of the data, calculate the Gini index of the two child nodes and use this to calculate the information gain.\n",
    "    - If none of the possible splits has an information gain of >0.02, terminate that branch or select the split with the highest information gain.\n",
    "    - Repeat this for each branch until it terminates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3233c27-f83f-4c76-8082-d8f536bb90a8",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Programming Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "080a97ea-2c7b-46ad-8d97-81e9eb8bbfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth 0: Node = [10, 10, 10] with Entropy IG = 0.6583, Gini IG = 0.25, Entropy = 1.585, Gini = 0.6667\n",
      "  Depth 1: Leaf node with values = [1, 1, 10], Entropy = 0.8167, Gini = 0.2917\n",
      "  Depth 1: Node = [9, 9, 0] with Entropy IG = 0.7394, Gini IG = 0.4, Entropy = 1.0, Gini = 0.5\n",
      "    Depth 2: Leaf node with values = [8, 0, 0], Entropy = -0.0, Gini = 0.0\n",
      "    Depth 2: Leaf node with values = [1, 9, 0], Entropy = 0.469, Gini = 0.18\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gini_index(p):\n",
    "    p = np.array(p, dtype=np.float64)\n",
    "    # Ensure correct probability distribution\n",
    "    p = p / np.sum(p)\n",
    "    p = p[p > 0] \n",
    "    # Compute gini index\n",
    "    return np.sum(p * (1-p))\n",
    "    \n",
    "def entropy(p):\n",
    "    p = np.array(p, dtype=np.float64)\n",
    "    # Ensure correct probability distribution\n",
    "    p = p / np.sum(p)\n",
    "    p = p[p > 0] \n",
    "    # Compute entropy\n",
    "    return -np.sum(p * np.log2(p))\n",
    "    \n",
    "def information_gain(root, children):\n",
    "    # Convert to numpy\n",
    "    root = np.array(root, dtype=np.float64)\n",
    "    # Compute entropy, gini for root H(N)\n",
    "    root_entropy = entropy(root)\n",
    "    root_gini = gini_index(root)\n",
    "    # Compute distribution\n",
    "    total = root.sum()\n",
    "    # Computer child entrophy w_i*H(N_i)... w_n*H(N_n), w_i = child / total\n",
    "    child_entropy = sum((sum(child) / total) * entropy(child) for child in children)\n",
    "    child_gini = sum((sum(child) / total) * gini_index(child) for child in children)\n",
    "    # Compute information gain\n",
    "    return root_entropy - child_entropy, root_gini - child_gini\n",
    "\n",
    "def traverse_tree(node, depth=0):\n",
    "    indent = \"  \" * depth\n",
    "    \n",
    "    # Compute entropy and gini index\n",
    "    ent = entropy(node['values'])\n",
    "    gini = gini_index(node['values'])\n",
    "    \n",
    "    # Check if node has children\n",
    "    if 'children' in node and node['children']:\n",
    "        children_values = [child['values'] for child in node['children']]\n",
    "        entropy_ig, gini_ig = information_gain(node['values'], children_values)        \n",
    "        print(f\"{indent}Depth {depth}: Node = {node['values']} with Entropy IG = {round(entropy_ig, 4)}, Gini IG = {round(gini_ig, 4)}, Entropy = {round(ent, 4)}, Gini = {round(gini, 4)}\")\n",
    "        # Recursively traverse children\n",
    "        for child in node['children']:\n",
    "            traverse_tree(child, depth + 1)\n",
    "    else: \n",
    "        print(f\"{indent}Depth {depth}: Leaf node with values = {node['values']}, Entropy = {round(ent, 4)}, Gini = {round(gini, 4)}\")\n",
    "\n",
    "tree = {\n",
    "    'values': [10, 10, 10],\n",
    "    'children': [ \n",
    "        {\n",
    "            'values': [1, 1, 10]\n",
    "        },\n",
    "        {\n",
    "            'values': [9, 9, 0],\n",
    "            'children': [\n",
    "                {\n",
    "                    'values': [8, 0, 0]\n",
    "                },\n",
    "                {\n",
    "                    'values': [1, 9, 0]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "        \n",
    "}\n",
    "\n",
    "\n",
    "traverse_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35b7e18c-a2fa-46dd-9bfb-ab19b0f4bbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Buys: 14, Total Doesn't Buy: 16\n",
      "Root Entropy: 0.9968\n",
      "Root Gini: 0.4978\n",
      "Split on Age -> Entropy IG: 0.0495, Gini IG: 0.0338\n",
      "Split on Hot/Cold -> Entropy IG: 0.5369, Gini IG: 0.32\n"
     ]
    }
   ],
   "source": [
    "data = np.array([\n",
    "    # (Buys, DoesNotBuy)\n",
    "    [1, 5], # '<30', 'hot', \n",
    "    [7, 0], # '<30', 'cold'\n",
    "    [0, 9], # '>30', 'hot'\n",
    "    [6, 2], # '>30', 'cold'\n",
    "])\n",
    "\n",
    "# Get total counts\n",
    "total_buys = np.sum(data[:, 0])\n",
    "total_not_buys = np.sum(data[:, 1])\n",
    "print(f\"Total Buys: {total_buys}, Total Doesn't Buy: {total_not_buys}\")\n",
    "\n",
    "parent_counts = [total_buys, total_not_buys]\n",
    "parent_entropy = entropy(parent_counts)\n",
    "print(f\"Root Entropy: {round(parent_entropy, 4)}\")\n",
    "\n",
    "parent_gini = gini_index(parent_counts)\n",
    "print(f\"Root Gini: {round(parent_gini, 4)}\")\n",
    "\n",
    "# Compute values - age\n",
    "split_age = (np.sum(data[:2], axis=0), np.sum(data[2:], axis=0))\n",
    "ig_entropy, ig_gini = information_gain(parent_counts, split_age)\n",
    "print(f\"Split on Age -> Entropy IG: {np.round(ig_entropy, 4)}, Gini IG: {np.round(ig_gini, 4)}\")\n",
    "\n",
    "# Compute values - hot/cold\n",
    "split_temp = (data[0] + data[2], data[1] + data[3])\n",
    "ig_entropy, ig_gini = information_gain(parent_counts, split_temp)\n",
    "print(f\"Split on Hot/Cold -> Entropy IG: {np.round(ig_entropy, 4)}, Gini IG: {np.round(ig_gini, 4)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BaseGPU",
   "language": "python",
   "name": "basegpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
