{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94789ec-5081-4692-bde2-aa5869467b9c",
   "metadata": {},
   "source": [
    "# Module 19 - Hyperparemters\n",
    "\n",
    "## Learning outcomes\n",
    "\n",
    "- LO 1: Dissect a simple neural network using the PyTorch optimisation framework.\n",
    "- LO 2: Evaluate the trade-offs of employing deep neural networks in your machine learning projects.\n",
    "- LO 3: Refine an existing neural network for a specific application.\n",
    "- LO 4: Develop intuition on the structure and components of a neural network.\n",
    "- LO 5: Refine a codebase for machine learning competitions.\n",
    "\n",
    "## Misc and Keywords\n",
    "- **Gradient descent** with momentum is designed to help the gradient descent algorithm converge faster.\n",
    "- **Fully connected** means that there is an edge, or weight, between any two nodes.\n",
    "- **Torch tensor** is the same as numpy array, an *n-dimensional* array, except it can run on GPUs\n",
    "- In PyTorch, the **forward function** defines the computation that the model performs on input data to produce output.\n",
    "- **Tokenisation** is simply the process of breaking a text sample into words.\n",
    "\n",
    "## Module Summary Description\n",
    "One of the keys to building a neural network is the various design choices you must make along the way. Alex explains that these design parameters, commonly known as hyperparameters, are like barcodes for different models on your model shelf. In this module, you’ll learn about hyperparameter optimization, which will help you to choose the best model for a given task.\n",
    "\n",
    "#### Learn PyTorch in 60 minutes: \n",
    "https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01039797-d903-4603-921b-d75fb005d7f5",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "In deep learning, **hyperparameters** are configuration settings used to control the training process of a model.  \n",
    "Unlike model parameters (which are learned during training), hyperparameters are set before training begins and influence how learning happens.\n",
    "\n",
    "### Why Hyperparameters Matter\n",
    "\n",
    "Hyperparameters impact:\n",
    "- **Model performance:** Poor choices can lead to underfitting or overfitting.\n",
    "- **Training time:** Some settings speed up or slow down convergence.\n",
    "- **Resource usage:** Affects memory, CPU/GPU usage, etc.\n",
    "\n",
    "### Examples of hyperparamter choices:\n",
    "\n",
    "- Which step size of the stochastic gradient descent (SGD), $s$, to use\n",
    "- Which batch size, $B$, to use for the SGB\n",
    "- Which dropout rate, $r$, for regularisation\n",
    "- Which weight decay to regularise? and at which rate?\n",
    "- Which loss function to use? Quadratic? cross-entropy?\n",
    "\n",
    "Hyperparameters are not the same as the weights in the network which are learnt through gradient descent, they instead need to be defined by the user.\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "#### Learning Rate (`lr`)\n",
    "\n",
    "Controls how much to change the model in response to the estimated error each time the model weights are updated.\n",
    "\n",
    "**Trade-off:**  \n",
    "- Too high → unstable training, may diverge.\n",
    "- Too low → slow convergence or stuck in local minima.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "```\n",
    "\n",
    "#### Batch Size\n",
    " \n",
    "Number of samples processed before the model updates.\n",
    "\n",
    "**Trade-off:**  \n",
    "- Small → noisy updates but better generalisation.\n",
    "- Large → stable updates but risk of poorer generalisation.\n",
    "\n",
    "```python\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=10)\n",
    "```\n",
    "\n",
    "#### Number of Epochs\n",
    "\n",
    "Number of complete passes through the entire training dataset.\n",
    "\n",
    "**Trade-off:**  \n",
    "- Too few → underfitting.\n",
    "- Too many → overfitting.\n",
    "\n",
    "```python\n",
    "model.fit(X_train, y_train, epochs=50)\n",
    "```\n",
    "\n",
    "#### Optimiser Type\n",
    "\n",
    "Algorithm used to update network weights.\n",
    "\n",
    "**Examples:**  \n",
    "- SGD (Stochastic Gradient Descent) → simple but may struggle with complex landscapes.\n",
    "- Adam → adaptive learning rate, works well in most cases.\n",
    "- RMSprop → works well with recurrent networks.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "optimizer = SGD(learning_rate=0.01)\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "```\n",
    "\n",
    "#### Dropout Rate\n",
    " \n",
    "Fraction of neurons randomly set to zero during training to prevent overfitting.\n",
    "\n",
    "**Trade-off:**  \n",
    "- Low dropout → risk of overfitting.\n",
    "- High dropout → risk of underfitting.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "```\n",
    "\n",
    "#### Weight Initialisation\n",
    " \n",
    "How initial values for weights are set before training.\n",
    "\n",
    "**Trade-off:**  \n",
    "- Poor initialisation → slow or failed convergence.\n",
    "- Good initialisation (e.g., He or Xavier) → faster and stable training.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "\n",
    "model.add(Dense(64, kernel_initializer=HeNormal()))\n",
    "```\n",
    "\n",
    "#### Regularisation (L1, L2)\n",
    "\n",
    "Penalties added to the loss function to discourage large weights and prevent overfitting.\n",
    "\n",
    "**Trade-off:**  \n",
    "- Too strong → underfitting.\n",
    "- Too weak → overfitting.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "model.add(Dense(64, kernel_regularizer=l2(0.01)))\n",
    "```\n",
    "\n",
    "#### Early Stopping\n",
    "  \n",
    "Stops training when the model performance stops improving on validation data.\n",
    "\n",
    "**Trade-off:**  \n",
    "- Helps avoid overfitting.\n",
    "- If used improperly, may stop too early (underfitting).\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, callbacks=[early_stop])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b94a29-55d7-429f-b1b9-a991171c2c45",
   "metadata": {},
   "source": [
    "## Introduction to torch\n",
    "\n",
    "#### Torch Tensors\n",
    "A **Torch tensor** is the same as numpy array, an *n-dimensional* array, except: \n",
    "- It can run on GPUs or other accelerators (with .to(device) or .cuda()), while NumPy is CPU-only.\n",
    "- It supports automatic differentiation (with .requires_grad=True), which allows gradients to be calculated and used for optimization (NumPy does not support this natively).\n",
    "- It integrates seamlessly with PyTorch's neural network modules and optimizers.\n",
    "- It has extra functionalities specific to deep learning (e.g., in-place ops, mixed precision, hooks).\n",
    "\n",
    "An example is shown below\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# NumPy array\n",
    "np_array = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "# Torch tensor\n",
    "tensor = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# GPU tensor (optional)\n",
    "tensor_gpu = tensor.to(\"cuda\")  # if GPU available\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbf0b3c-1f58-4777-ad86-df58a44c966c",
   "metadata": {},
   "source": [
    "#### What is *requires_grad* in PyTorch?\n",
    "In PyTorch, every Tensor has a flag called requires_grad.\n",
    "When set to True, PyTorch tracks all operations on the tensor so that it can automatically compute gradients during backpropagation.\n",
    "\n",
    "This enables the use of automatic differentiation (autograd), which is essential for training neural networks.\n",
    "Tensors with *requires_grad=True* will accumulate gradients in their .grad attribute after .backward() is called.\n",
    "\n",
    "##### How it works:\n",
    "- Forward pass: PyTorch records the operations in a computational graph.\n",
    "- Backward pass (.backward()): PyTorch traverses this graph in reverse, applying the chain rule to compute gradients.\n",
    "- Result: Each tensor with requires_grad=True will have its .grad populated with the gradient of the loss with respect to that tensor.\n",
    "\n",
    "##### Why is this useful?\n",
    "Gradients indicate how much a small change in the tensor affects the loss.\n",
    "\n",
    "Optimizers like torch.optim.SGD use these gradients to update parameters and minimize the loss.\n",
    "\n",
    "##### Preventing gradient tracking (*torch.no_grad*)\n",
    "In scenarios where gradient computation is unnecessary, such as model evaluation or inference, we can temporarily disable gradient tracking using torch.no_grad()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ef357d9-ecc1-4a4b-9a9e-472ed8a0f650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Tensor with gradients enabled\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "# Operation\n",
    "y = x * 2\n",
    "\n",
    "# Backward pass\n",
    "y.sum().backward()\n",
    "\n",
    "print(x.grad)  # Shows gradient of y.sum() w.r.t. x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab206abc-0942-4d62-b4f9-4e0716693a7d",
   "metadata": {},
   "source": [
    "---\n",
    "## Vanilla NumPy Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7241d60e-edab-4df0-9c45-1c2cfc656d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 25344617.80024845\n",
      "1 19241213.077182896\n",
      "2 16542566.454395559\n",
      "3 14849562.121751975\n",
      "4 13073525.812466811\n",
      "5 10990722.99077887\n",
      "6 8671290.910533313\n",
      "7 6490916.280778909\n",
      "8 4642667.820183601\n",
      "9 3253403.8974768245\n",
      "10 2261362.1082796818\n",
      "11 1588702.3658847143\n",
      "12 1137093.229593238\n",
      "13 836452.3221806737\n",
      "14 633186.1476644496\n",
      "15 493606.2001597121\n",
      "16 395103.45354633604\n",
      "17 323644.12690757733\n",
      "18 270139.9049382714\n",
      "19 228924.04802692542\n",
      "20 196377.3004188963\n",
      "21 170141.81240083894\n",
      "22 148531.91507549188\n",
      "23 130457.2192998358\n",
      "24 115151.67019756013\n",
      "25 102049.25741051119\n",
      "26 90751.39043669734\n",
      "27 80947.29451531041\n",
      "28 72400.74906637982\n",
      "29 64911.51236956321\n",
      "30 58330.32798915943\n",
      "31 52519.47797378483\n",
      "32 47379.431599016825\n",
      "33 42837.71283637645\n",
      "34 38794.97972227994\n",
      "35 35192.964329675306\n",
      "36 31975.509668982988\n",
      "37 29092.521114024465\n",
      "38 26505.626401555837\n",
      "39 24179.97715220185\n",
      "40 22089.419774448073\n",
      "41 20205.548797438147\n",
      "42 18503.3526295163\n",
      "43 16963.30617017058\n",
      "44 15568.402758653636\n",
      "45 14303.03564684394\n",
      "46 13153.310028642969\n",
      "47 12107.752241122946\n",
      "48 11155.055072508008\n",
      "49 10286.09674497856\n",
      "50 9492.970422315093\n",
      "51 8769.238681108916\n",
      "52 8107.550613994268\n",
      "53 7501.408166039526\n",
      "54 6945.599053391644\n",
      "55 6435.713937224098\n",
      "56 5967.162151675216\n",
      "57 5536.51970522239\n",
      "58 5140.202231014329\n",
      "59 4775.240406118776\n",
      "60 4439.365630210966\n",
      "61 4129.390957566136\n",
      "62 3843.3337939155426\n",
      "63 3579.042460867718\n",
      "64 3334.6336127833197\n",
      "65 3108.485532262238\n",
      "66 2898.9817143190903\n",
      "67 2704.860640334676\n",
      "68 2524.632658313604\n",
      "69 2357.5631461360654\n",
      "70 2202.5057722621887\n",
      "71 2058.4945293746687\n",
      "72 1924.6320512893658\n",
      "73 1800.2954533987613\n",
      "74 1684.537852126689\n",
      "75 1576.7966770455564\n",
      "76 1476.4594732356086\n",
      "77 1383.0431489895245\n",
      "78 1295.9293453860046\n",
      "79 1214.6828434914148\n",
      "80 1138.9569800375818\n",
      "81 1068.5148725677118\n",
      "82 1002.7636086791777\n",
      "83 941.3223679274147\n",
      "84 883.8832631164598\n",
      "85 830.1846828523572\n",
      "86 779.9658378867503\n",
      "87 732.9741099064622\n",
      "88 688.9708867709592\n",
      "89 647.7928251834119\n",
      "90 609.2129238877081\n",
      "91 573.0913077785425\n",
      "92 539.2440804052236\n",
      "93 507.52561227201153\n",
      "94 477.7618113494703\n",
      "95 449.8540106507728\n",
      "96 423.65776414506263\n",
      "97 399.0726520760692\n",
      "98 375.98303783767017\n",
      "99 354.3053512553542\n",
      "100 333.9430282640989\n",
      "101 314.80461124390695\n",
      "102 296.8208161317476\n",
      "103 279.9203330942831\n",
      "104 264.02706456918827\n",
      "105 249.07267273486983\n",
      "106 234.99971714061067\n",
      "107 221.7600674025322\n",
      "108 209.3050162627918\n",
      "109 197.57610390601943\n",
      "110 186.53385210124662\n",
      "111 176.13763557672468\n",
      "112 166.35110100698077\n",
      "113 157.1294369571827\n",
      "114 148.4419222668816\n",
      "115 140.25321527694098\n",
      "116 132.53652936330002\n",
      "117 125.25845843511175\n",
      "118 118.39324835626932\n",
      "119 111.92009777108619\n",
      "120 105.81664174043368\n",
      "121 100.05773326808341\n",
      "122 94.62098136163175\n",
      "123 89.49568764191599\n",
      "124 84.65942638204025\n",
      "125 80.09061822344562\n",
      "126 75.776240183293\n",
      "127 71.70172457952593\n",
      "128 67.8556665941783\n",
      "129 64.22077584948948\n",
      "130 60.78966315843054\n",
      "131 57.54482682832861\n",
      "132 54.47899179188058\n",
      "133 51.580339580436636\n",
      "134 48.84122427666985\n",
      "135 46.252060018083135\n",
      "136 43.80456704349793\n",
      "137 41.4885924721587\n",
      "138 39.298858501681714\n",
      "139 37.22829508621058\n",
      "140 35.270844115089\n",
      "141 33.41752890971631\n",
      "142 31.664073527617923\n",
      "143 30.00506106544252\n",
      "144 28.435630086332278\n",
      "145 26.94989471559709\n",
      "146 25.544070021139305\n",
      "147 24.212950683941898\n",
      "148 22.953925441117256\n",
      "149 21.761739080254227\n",
      "150 20.632205104921653\n",
      "151 19.562820132925726\n",
      "152 18.55037937265325\n",
      "153 17.59108960245814\n",
      "154 16.68241571253294\n",
      "155 15.821725507714028\n",
      "156 15.006762360966585\n",
      "157 14.234481363443896\n",
      "158 13.502904680614114\n",
      "159 12.809486422636567\n",
      "160 12.152533087854614\n",
      "161 11.529850416982537\n",
      "162 10.939664589509897\n",
      "163 10.380326370739002\n",
      "164 9.85020838783701\n",
      "165 9.347605755985365\n",
      "166 8.871040095899327\n",
      "167 8.419596491878824\n",
      "168 7.991272729978558\n",
      "169 7.585045627621225\n",
      "170 7.199925353285083\n",
      "171 6.834725149723543\n",
      "172 6.488363044704589\n",
      "173 6.159721461660574\n",
      "174 5.848040827316666\n",
      "175 5.552520531256283\n",
      "176 5.272235623039137\n",
      "177 5.006200595859703\n",
      "178 4.75379482481369\n",
      "179 4.514407255064954\n",
      "180 4.287135081802416\n",
      "181 4.071501820446953\n",
      "182 3.8669339246645293\n",
      "183 3.6728185077735307\n",
      "184 3.488507670382236\n",
      "185 3.313667943871114\n",
      "186 3.1477257118980777\n",
      "187 2.990190492547335\n",
      "188 2.840604257182564\n",
      "189 2.698629517792003\n",
      "190 2.563849028505115\n",
      "191 2.435891607526085\n",
      "192 2.314403531636477\n",
      "193 2.199061457537997\n",
      "194 2.0896263659945356\n",
      "195 1.9856653382624372\n",
      "196 1.886909028074553\n",
      "197 1.7931191091745553\n",
      "198 1.7040726699939008\n",
      "199 1.6194691359318834\n",
      "200 1.5391291243442335\n",
      "201 1.462833450652864\n",
      "202 1.3903795254156837\n",
      "203 1.321536453312969\n",
      "204 1.2561563999075847\n",
      "205 1.1940383824509284\n",
      "206 1.1350522678114872\n",
      "207 1.0789843421250644\n",
      "208 1.0257097289033548\n",
      "209 0.9751104383122617\n",
      "210 0.9270382111618404\n",
      "211 0.8813483952409739\n",
      "212 0.8379309039021547\n",
      "213 0.7967046661523319\n",
      "214 0.7575152490523251\n",
      "215 0.7202659776774032\n",
      "216 0.6848665698827087\n",
      "217 0.6512332200465141\n",
      "218 0.6192564488207439\n",
      "219 0.5888648272070123\n",
      "220 0.5599839830395223\n",
      "221 0.5325407280521942\n",
      "222 0.5064509105321056\n",
      "223 0.48165275303524097\n",
      "224 0.45807709073520275\n",
      "225 0.4356702153276625\n",
      "226 0.4143620815428559\n",
      "227 0.3941054006988175\n",
      "228 0.3748545793085961\n",
      "229 0.356552135926902\n",
      "230 0.3391454442990621\n",
      "231 0.32259800895487\n",
      "232 0.306875540084669\n",
      "233 0.2919157500722951\n",
      "234 0.2776912011707594\n",
      "235 0.26416742260821663\n",
      "236 0.2513096245722869\n",
      "237 0.23907817296065664\n",
      "238 0.22744970732526287\n",
      "239 0.2163904453320507\n",
      "240 0.20587515925026678\n",
      "241 0.1958746622494925\n",
      "242 0.18636445324815687\n",
      "243 0.17731839424079204\n",
      "244 0.16871544417098766\n",
      "245 0.1605309400020803\n",
      "246 0.15274844699257994\n",
      "247 0.14535197387041981\n",
      "248 0.1383105485790446\n",
      "249 0.13161111650343627\n",
      "250 0.12523865066395518\n",
      "251 0.11918111083714605\n",
      "252 0.11341617719720701\n",
      "253 0.10793099825954776\n",
      "254 0.10271300215943277\n",
      "255 0.09775065719120615\n",
      "256 0.09302894280574356\n",
      "257 0.08853623818097796\n",
      "258 0.08426192436831477\n",
      "259 0.08019708929042882\n",
      "260 0.07632886489298576\n",
      "261 0.07264868759542412\n",
      "262 0.06914601156844694\n",
      "263 0.06581452909816944\n",
      "264 0.06264343263999547\n",
      "265 0.05962605097328347\n",
      "266 0.05675540350157228\n",
      "267 0.05402485843694101\n",
      "268 0.051424972571317284\n",
      "269 0.04895144053864112\n",
      "270 0.046598286773140374\n",
      "271 0.044358776150681914\n",
      "272 0.042226762332785536\n",
      "273 0.04019828333628944\n",
      "274 0.03826780146752033\n",
      "275 0.03643063058548639\n",
      "276 0.03468173137769147\n",
      "277 0.03301746642193763\n",
      "278 0.031433868011438104\n",
      "279 0.02992657314290046\n",
      "280 0.02849220247358536\n",
      "281 0.027126373682854435\n",
      "282 0.025826901324602455\n",
      "283 0.024589743550617035\n",
      "284 0.023411930758227693\n",
      "285 0.022291032067106944\n",
      "286 0.021224469820523853\n",
      "287 0.02020881946462142\n",
      "288 0.019241923870287017\n",
      "289 0.018321572216353305\n",
      "290 0.01744617800625857\n",
      "291 0.01661228085321573\n",
      "292 0.015818438192067928\n",
      "293 0.015063318081699934\n",
      "294 0.014344488279935265\n",
      "295 0.01365961426509936\n",
      "296 0.013007531829835196\n",
      "297 0.012386876025276454\n",
      "298 0.011796098778065435\n",
      "299 0.011233512010277833\n",
      "300 0.010697956917198787\n",
      "301 0.010187939283155933\n",
      "302 0.009702500500462709\n",
      "303 0.009240215003070408\n",
      "304 0.008800046033499213\n",
      "305 0.008380928389121613\n",
      "306 0.007981941336123247\n",
      "307 0.007601958734225413\n",
      "308 0.007240127390272756\n",
      "309 0.0068956733932958435\n",
      "310 0.006567804839262117\n",
      "311 0.006255400811633886\n",
      "312 0.0059579540448454005\n",
      "313 0.0056746689969094455\n",
      "314 0.005405027689176195\n",
      "315 0.005148182762304597\n",
      "316 0.004903589998874266\n",
      "317 0.004670648299525465\n",
      "318 0.00444889661318488\n",
      "319 0.0042376874213713635\n",
      "320 0.004036566009904513\n",
      "321 0.0038449920650251244\n",
      "322 0.003662590272752689\n",
      "323 0.0034888238963819173\n",
      "324 0.003323347763732893\n",
      "325 0.0031657299438220907\n",
      "326 0.003015675467831465\n",
      "327 0.0028727344544988358\n",
      "328 0.002736589659057061\n",
      "329 0.002606924540609082\n",
      "330 0.002483506020193605\n",
      "331 0.0023658696013756616\n",
      "332 0.0022538287730346582\n",
      "333 0.0021471182166330404\n",
      "334 0.002045523144170963\n",
      "335 0.0019487581461364188\n",
      "336 0.0018565891421267224\n",
      "337 0.0017687513925987975\n",
      "338 0.0016851047059795504\n",
      "339 0.0016054214666774234\n",
      "340 0.0015295199250583323\n",
      "341 0.0014572090417437978\n",
      "342 0.001388340815784042\n",
      "343 0.0013227234465461256\n",
      "344 0.0012602261170905219\n",
      "345 0.00120069162557519\n",
      "346 0.0011439836407307366\n",
      "347 0.0010899569714697811\n",
      "348 0.0010384912172926705\n",
      "349 0.0009894676857072362\n",
      "350 0.0009427770164212532\n",
      "351 0.0008982903737089733\n",
      "352 0.0008559003361775977\n",
      "353 0.0008155166794351906\n",
      "354 0.0007770527057917236\n",
      "355 0.0007404085358968792\n",
      "356 0.0007054944140179002\n",
      "357 0.0006722309665054304\n",
      "358 0.0006405436668553766\n",
      "359 0.0006103635871620691\n",
      "360 0.0005816064546684009\n",
      "361 0.0005542025373123571\n",
      "362 0.0005280959025404932\n",
      "363 0.0005032257784428594\n",
      "364 0.0004795256897154867\n",
      "365 0.000456945367023611\n",
      "366 0.00043542895581412557\n",
      "367 0.00041493681405797844\n",
      "368 0.0003954090668513604\n",
      "369 0.0003768020618884873\n",
      "370 0.00035907453530635135\n",
      "371 0.0003421849799938618\n",
      "372 0.0003260908830456841\n",
      "373 0.00031075299950101794\n",
      "374 0.0002961398218998438\n",
      "375 0.0002822307724656716\n",
      "376 0.00026896829996476246\n",
      "377 0.0002563246170256524\n",
      "378 0.000244276948516748\n",
      "379 0.00023279997649264491\n",
      "380 0.00022186754538280947\n",
      "381 0.00021144444959961291\n",
      "382 0.0002015128468883507\n",
      "383 0.0001920485321985285\n",
      "384 0.00018303314064410677\n",
      "385 0.00017443948352074374\n",
      "386 0.0001662497357561011\n",
      "387 0.00015844501472378537\n",
      "388 0.00015101117566740092\n",
      "389 0.00014392450539551027\n",
      "390 0.00013717254793732883\n",
      "391 0.00013073790543442526\n",
      "392 0.0001246064119494906\n",
      "393 0.00011876237160341\n",
      "394 0.00011319220420086695\n",
      "395 0.000107884471226299\n",
      "396 0.00010282667279551142\n",
      "397 9.800754303506644e-05\n",
      "398 9.341339445569804e-05\n",
      "399 8.903518787654267e-05\n",
      "400 8.486381917525026e-05\n",
      "401 8.088878608478783e-05\n",
      "402 7.709926289915353e-05\n",
      "403 7.348794953032269e-05\n",
      "404 7.004604491389964e-05\n",
      "405 6.67660414258575e-05\n",
      "406 6.363977138851654e-05\n",
      "407 6.065986025500906e-05\n",
      "408 5.782021523009482e-05\n",
      "409 5.511372431820924e-05\n",
      "410 5.25350643250747e-05\n",
      "411 5.007682022232295e-05\n",
      "412 4.773360180621722e-05\n",
      "413 4.550275540923499e-05\n",
      "414 4.337461436759783e-05\n",
      "415 4.1345876975511895e-05\n",
      "416 3.9411845488366364e-05\n",
      "417 3.75684765969625e-05\n",
      "418 3.5811917326427255e-05\n",
      "419 3.413778188609747e-05\n",
      "420 3.254194171828779e-05\n",
      "421 3.102098058574319e-05\n",
      "422 2.9571071226839217e-05\n",
      "423 2.8189509939874643e-05\n",
      "424 2.687207150219395e-05\n",
      "425 2.5616419037655998e-05\n",
      "426 2.4419568381414584e-05\n",
      "427 2.3278957767996426e-05\n",
      "428 2.2191668317372145e-05\n",
      "429 2.1155061538082972e-05\n",
      "430 2.016705367395541e-05\n",
      "431 1.9225724836087744e-05\n",
      "432 1.832822209524888e-05\n",
      "433 1.74724279243417e-05\n",
      "434 1.665680947857898e-05\n",
      "435 1.587924432648389e-05\n",
      "436 1.5138178413826749e-05\n",
      "437 1.4431789196266099e-05\n",
      "438 1.3758377347285508e-05\n",
      "439 1.311634216055911e-05\n",
      "440 1.2504431486185978e-05\n",
      "441 1.1921441821381326e-05\n",
      "442 1.1365374821233767e-05\n",
      "443 1.0835261345465795e-05\n",
      "444 1.0329934625484213e-05\n",
      "445 9.848303608039154e-06\n",
      "446 9.389105857716692e-06\n",
      "447 8.951324905210083e-06\n",
      "448 8.534006684425884e-06\n",
      "449 8.136225286750626e-06\n",
      "450 7.757495401065118e-06\n",
      "451 7.396068585297516e-06\n",
      "452 7.051430371144347e-06\n",
      "453 6.722839729840229e-06\n",
      "454 6.409656655919953e-06\n",
      "455 6.111051506044774e-06\n",
      "456 5.8263558419895e-06\n",
      "457 5.554964967999139e-06\n",
      "458 5.2962335220235116e-06\n",
      "459 5.04962859620479e-06\n",
      "460 4.814490096980484e-06\n",
      "461 4.590304087932446e-06\n",
      "462 4.376611154220796e-06\n",
      "463 4.172867391084318e-06\n",
      "464 3.978640430575841e-06\n",
      "465 3.793418510113385e-06\n",
      "466 3.6168550078737005e-06\n",
      "467 3.4485182809213405e-06\n",
      "468 3.288067965887199e-06\n",
      "469 3.135055028298762e-06\n",
      "470 2.989172816626838e-06\n",
      "471 2.850097017451811e-06\n",
      "472 2.7175449182692408e-06\n",
      "473 2.5911587713194237e-06\n",
      "474 2.4706444170356086e-06\n",
      "475 2.3557318680664774e-06\n",
      "476 2.2461567669134575e-06\n",
      "477 2.1417196580489184e-06\n",
      "478 2.0421459383356223e-06\n",
      "479 1.9471858187098388e-06\n",
      "480 1.8566470086463617e-06\n",
      "481 1.770328987658239e-06\n",
      "482 1.6880647803498497e-06\n",
      "483 1.6096112870682217e-06\n",
      "484 1.5348017709968934e-06\n",
      "485 1.4635207190671832e-06\n",
      "486 1.3955518541774088e-06\n",
      "487 1.3307154364886117e-06\n",
      "488 1.2688758338677308e-06\n",
      "489 1.2099271173330881e-06\n",
      "490 1.1537209465079324e-06\n",
      "491 1.1001313202748734e-06\n",
      "492 1.0490288989984679e-06\n",
      "493 1.0003188840986991e-06\n",
      "494 9.538559979998201e-07\n",
      "495 9.095545897622289e-07\n",
      "496 8.673291936809827e-07\n",
      "497 8.270533539111131e-07\n",
      "498 7.886470497959302e-07\n",
      "499 7.520290449916698e-07\n",
      "This is the shape of w2: (100, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension or number of features;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in) \n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "\n",
    "print('This is the shape of w2:', w2.shape)\n",
    "\n",
    "# this is the neural network\n",
    "def simple_nn(x):\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657e1339-bd34-431b-b969-314b2370816b",
   "metadata": {},
   "source": [
    "---\n",
    "# Pytorch Version\n",
    "Exactly the same model, but created in pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8908222c-095a-460a-80b9-a99beaa08831",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 680.4976806640625\n",
      "1 624.5360717773438\n",
      "2 577.7935180664062\n",
      "3 537.4030151367188\n",
      "4 502.2336730957031\n",
      "5 471.02789306640625\n",
      "6 442.8564453125\n",
      "7 417.1272277832031\n",
      "8 393.5264587402344\n",
      "9 371.5996398925781\n",
      "10 351.2056884765625\n",
      "11 332.0414733886719\n",
      "12 314.080078125\n",
      "13 297.07354736328125\n",
      "14 280.8624267578125\n",
      "15 265.52313232421875\n",
      "16 250.883056640625\n",
      "17 237.0072021484375\n",
      "18 223.77545166015625\n",
      "19 211.11370849609375\n",
      "20 199.01454162597656\n",
      "21 187.4600830078125\n",
      "22 176.50665283203125\n",
      "23 166.07212829589844\n",
      "24 156.16192626953125\n",
      "25 146.7483673095703\n",
      "26 137.82666015625\n",
      "27 129.40196228027344\n",
      "28 121.45137786865234\n",
      "29 113.9508285522461\n",
      "30 106.88719177246094\n",
      "31 100.21592712402344\n",
      "32 93.94758605957031\n",
      "33 88.05029296875\n",
      "34 82.52310943603516\n",
      "35 77.31945037841797\n",
      "36 72.44427490234375\n",
      "37 67.87490844726562\n",
      "38 63.600650787353516\n",
      "39 59.59953308105469\n",
      "40 55.85410690307617\n",
      "41 52.354244232177734\n",
      "42 49.082672119140625\n",
      "43 46.025733947753906\n",
      "44 43.160709381103516\n",
      "45 40.480628967285156\n",
      "46 37.97616195678711\n",
      "47 35.63505172729492\n",
      "48 33.444000244140625\n",
      "49 31.400840759277344\n",
      "50 29.497194290161133\n",
      "51 27.714746475219727\n",
      "52 26.04681396484375\n",
      "53 24.486753463745117\n",
      "54 23.02472496032715\n",
      "55 21.65526008605957\n",
      "56 20.371932983398438\n",
      "57 19.17015266418457\n",
      "58 18.042598724365234\n",
      "59 16.98613929748535\n",
      "60 15.994538307189941\n",
      "61 15.064643859863281\n",
      "62 14.192496299743652\n",
      "63 13.374773025512695\n",
      "64 12.608115196228027\n",
      "65 11.88790225982666\n",
      "66 11.211524963378906\n",
      "67 10.576722145080566\n",
      "68 9.980063438415527\n",
      "69 9.419837951660156\n",
      "70 8.89339828491211\n",
      "71 8.398737907409668\n",
      "72 7.933206558227539\n",
      "73 7.496397018432617\n",
      "74 7.086337089538574\n",
      "75 6.699793338775635\n",
      "76 6.33670711517334\n",
      "77 5.994829177856445\n",
      "78 5.672942161560059\n",
      "79 5.369711875915527\n",
      "80 5.084251403808594\n",
      "81 4.815177917480469\n",
      "82 4.561578273773193\n",
      "83 4.322933197021484\n",
      "84 4.097655773162842\n",
      "85 3.8850417137145996\n",
      "86 3.6844351291656494\n",
      "87 3.4953982830047607\n",
      "88 3.3169007301330566\n",
      "89 3.148176670074463\n",
      "90 2.9889473915100098\n",
      "91 2.8384957313537598\n",
      "92 2.6963071823120117\n",
      "93 2.561920642852783\n",
      "94 2.434570789337158\n",
      "95 2.3146400451660156\n",
      "96 2.2014458179473877\n",
      "97 2.0943989753723145\n",
      "98 1.993119239807129\n",
      "99 1.89724862575531\n",
      "100 1.806519627571106\n",
      "101 1.720474362373352\n",
      "102 1.6390081644058228\n",
      "103 1.5618906021118164\n",
      "104 1.4887577295303345\n",
      "105 1.4193836450576782\n",
      "106 1.3535797595977783\n",
      "107 1.29116690158844\n",
      "108 1.2320525646209717\n",
      "109 1.1760773658752441\n",
      "110 1.1229667663574219\n",
      "111 1.0725399255752563\n",
      "112 1.024635672569275\n",
      "113 0.9791402816772461\n",
      "114 0.9358710050582886\n",
      "115 0.8947694897651672\n",
      "116 0.8556285500526428\n",
      "117 0.8184084296226501\n",
      "118 0.7829906940460205\n",
      "119 0.7492936849594116\n",
      "120 0.7172199487686157\n",
      "121 0.6867153644561768\n",
      "122 0.6577833890914917\n",
      "123 0.6302410364151001\n",
      "124 0.6040019392967224\n",
      "125 0.5789936184883118\n",
      "126 0.5551533102989197\n",
      "127 0.5324369668960571\n",
      "128 0.5107923746109009\n",
      "129 0.49014797806739807\n",
      "130 0.47044751048088074\n",
      "131 0.4516284763813019\n",
      "132 0.4336712062358856\n",
      "133 0.41654038429260254\n",
      "134 0.4001618027687073\n",
      "135 0.38449060916900635\n",
      "136 0.3695240020751953\n",
      "137 0.35515308380126953\n",
      "138 0.3414399325847626\n",
      "139 0.32832464575767517\n",
      "140 0.3157784640789032\n",
      "141 0.30376631021499634\n",
      "142 0.29226943850517273\n",
      "143 0.28127020597457886\n",
      "144 0.27073249220848083\n",
      "145 0.2606534957885742\n",
      "146 0.2509845495223999\n",
      "147 0.24172602593898773\n",
      "148 0.23284997045993805\n",
      "149 0.22433584928512573\n",
      "150 0.21617060899734497\n",
      "151 0.20834001898765564\n",
      "152 0.20083005726337433\n",
      "153 0.1936194747686386\n",
      "154 0.18670479953289032\n",
      "155 0.1800706386566162\n",
      "156 0.17369599640369415\n",
      "157 0.16757361590862274\n",
      "158 0.16169153153896332\n",
      "159 0.1560426503419876\n",
      "160 0.1506168693304062\n",
      "161 0.14539897441864014\n",
      "162 0.14038731157779694\n",
      "163 0.13556838035583496\n",
      "164 0.13093309104442596\n",
      "165 0.12647783756256104\n",
      "166 0.12218697369098663\n",
      "167 0.11806188523769379\n",
      "168 0.11408765614032745\n",
      "169 0.11026064306497574\n",
      "170 0.1065760925412178\n",
      "171 0.10302826017141342\n",
      "172 0.09961332380771637\n",
      "173 0.09632645547389984\n",
      "174 0.09315703064203262\n",
      "175 0.09010917693376541\n",
      "176 0.08716818690299988\n",
      "177 0.08433202654123306\n",
      "178 0.08159823715686798\n",
      "179 0.07896286994218826\n",
      "180 0.07642105221748352\n",
      "181 0.07397078722715378\n",
      "182 0.07160395383834839\n",
      "183 0.06930781900882721\n",
      "184 0.06709392368793488\n",
      "185 0.06495918333530426\n",
      "186 0.0628986656665802\n",
      "187 0.060907598584890366\n",
      "188 0.05898640304803848\n",
      "189 0.05713195726275444\n",
      "190 0.055341530591249466\n",
      "191 0.053612612187862396\n",
      "192 0.05194281041622162\n",
      "193 0.05033482611179352\n",
      "194 0.04878275468945503\n",
      "195 0.04728308692574501\n",
      "196 0.04583334922790527\n",
      "197 0.044431716203689575\n",
      "198 0.043077096343040466\n",
      "199 0.041767820715904236\n",
      "200 0.040501534938812256\n",
      "201 0.03927749767899513\n",
      "202 0.03809297829866409\n",
      "203 0.03694136068224907\n",
      "204 0.03582763671875\n",
      "205 0.03475137799978256\n",
      "206 0.033710777759552\n",
      "207 0.032703958451747894\n",
      "208 0.03172965720295906\n",
      "209 0.030786897987127304\n",
      "210 0.029874300584197044\n",
      "211 0.02899135835468769\n",
      "212 0.02813638374209404\n",
      "213 0.027308641001582146\n",
      "214 0.026507200673222542\n",
      "215 0.025731291621923447\n",
      "216 0.024980146437883377\n",
      "217 0.024252381175756454\n",
      "218 0.02355046756565571\n",
      "219 0.022876067087054253\n",
      "220 0.02222280390560627\n",
      "221 0.021590236574411392\n",
      "222 0.020977018401026726\n",
      "223 0.02038302645087242\n",
      "224 0.01980743184685707\n",
      "225 0.019249100238084793\n",
      "226 0.01870810240507126\n",
      "227 0.01818372681736946\n",
      "228 0.01767577789723873\n",
      "229 0.017183125019073486\n",
      "230 0.016704706475138664\n",
      "231 0.016240976750850677\n",
      "232 0.01579124480485916\n",
      "233 0.015353942289948463\n",
      "234 0.014929664321243763\n",
      "235 0.01451793685555458\n",
      "236 0.014118701219558716\n",
      "237 0.013731221668422222\n",
      "238 0.013355047442018986\n",
      "239 0.012989789247512817\n",
      "240 0.012635491788387299\n",
      "241 0.012291397899389267\n",
      "242 0.011957122944295406\n",
      "243 0.011632846668362617\n",
      "244 0.011317845433950424\n",
      "245 0.011012374423444271\n",
      "246 0.010715306736528873\n",
      "247 0.010426519438624382\n",
      "248 0.010145983658730984\n",
      "249 0.009873452596366405\n",
      "250 0.009608795866370201\n",
      "251 0.009351630695164204\n",
      "252 0.00910184159874916\n",
      "253 0.00885905884206295\n",
      "254 0.00862331222742796\n",
      "255 0.008393974974751472\n",
      "256 0.00817106757313013\n",
      "257 0.007954489439725876\n",
      "258 0.007743889465928078\n",
      "259 0.0075391377322375774\n",
      "260 0.007340374402701855\n",
      "261 0.0071469563990831375\n",
      "262 0.0069588422775268555\n",
      "263 0.006776046939194202\n",
      "264 0.006598335225135088\n",
      "265 0.006425537168979645\n",
      "266 0.006257481873035431\n",
      "267 0.006093961652368307\n",
      "268 0.005935006309300661\n",
      "269 0.005780386738479137\n",
      "270 0.005629869177937508\n",
      "271 0.00548358540982008\n",
      "272 0.005341263487935066\n",
      "273 0.005202835891395807\n",
      "274 0.005068186204880476\n",
      "275 0.004937115125358105\n",
      "276 0.0048096333630383015\n",
      "277 0.004685632884502411\n",
      "278 0.004564946983009577\n",
      "279 0.004447535611689091\n",
      "280 0.004333216696977615\n",
      "281 0.0042219688184559345\n",
      "282 0.004113730508834124\n",
      "283 0.004008377902209759\n",
      "284 0.0039058104157447815\n",
      "285 0.003806097898632288\n",
      "286 0.003709007753059268\n",
      "287 0.0036144384648650885\n",
      "288 0.003522403072565794\n",
      "289 0.003432775614783168\n",
      "290 0.0033455854281783104\n",
      "291 0.0032606786116957664\n",
      "292 0.0031780535355210304\n",
      "293 0.003097535576671362\n",
      "294 0.003019152209162712\n",
      "295 0.002942829392850399\n",
      "296 0.00286850705742836\n",
      "297 0.0027961109299212694\n",
      "298 0.0027256261091679335\n",
      "299 0.002656967146322131\n",
      "300 0.002590134972706437\n",
      "301 0.0025250380858778954\n",
      "302 0.002461598953232169\n",
      "303 0.0023998587857931852\n",
      "304 0.0023397421464323997\n",
      "305 0.0022811624221503735\n",
      "306 0.0022240797989070415\n",
      "307 0.002168472623452544\n",
      "308 0.0021143173798918724\n",
      "309 0.002061552368104458\n",
      "310 0.002010161057114601\n",
      "311 0.0019600766245275736\n",
      "312 0.001911292434670031\n",
      "313 0.0018637583125382662\n",
      "314 0.0018174265278503299\n",
      "315 0.0017723042983561754\n",
      "316 0.0017283426132053137\n",
      "317 0.0016855179565027356\n",
      "318 0.0016437822487205267\n",
      "319 0.00160310510545969\n",
      "320 0.0015634531155228615\n",
      "321 0.0015248339623212814\n",
      "322 0.0014871691819280386\n",
      "323 0.0014504794962704182\n",
      "324 0.0014147155452519655\n",
      "325 0.001379834022372961\n",
      "326 0.0013458502944558859\n",
      "327 0.0013127257116138935\n",
      "328 0.0012804409489035606\n",
      "329 0.001248967251740396\n",
      "330 0.0012183039216324687\n",
      "331 0.001188410446047783\n",
      "332 0.0011592572554945946\n",
      "333 0.0011308530811220407\n",
      "334 0.001103170681744814\n",
      "335 0.001076178508810699\n",
      "336 0.0010498517658561468\n",
      "337 0.0010241955751553178\n",
      "338 0.0009991824626922607\n",
      "339 0.0009747946169227362\n",
      "340 0.0009510029340162873\n",
      "341 0.0009278275538235903\n",
      "342 0.0009052428649738431\n",
      "343 0.0008832135936245322\n",
      "344 0.0008617158746346831\n",
      "345 0.0008407604182139039\n",
      "346 0.0008203247562050819\n",
      "347 0.0008004077826626599\n",
      "348 0.0007809874950908124\n",
      "349 0.0007620397955179214\n",
      "350 0.000743570679333061\n",
      "351 0.0007255615782923996\n",
      "352 0.0007079910137690604\n",
      "353 0.0006908548530191183\n",
      "354 0.0006741409888491035\n",
      "355 0.000657841912470758\n",
      "356 0.0006419424316845834\n",
      "357 0.0006264476105570793\n",
      "358 0.0006113150739111006\n",
      "359 0.0005965730524621904\n",
      "360 0.0005821892409585416\n",
      "361 0.0005681617185473442\n",
      "362 0.0005544725572690368\n",
      "363 0.000541148183401674\n",
      "364 0.0005281224730424583\n",
      "365 0.0005154258687980473\n",
      "366 0.0005030324682593346\n",
      "367 0.0004909522831439972\n",
      "368 0.0004791603423655033\n",
      "369 0.0004676674143411219\n",
      "370 0.0004564450355246663\n",
      "371 0.0004455021698959172\n",
      "372 0.0004348185029812157\n",
      "373 0.0004243994480930269\n",
      "374 0.0004142446559853852\n",
      "375 0.0004043299413751811\n",
      "376 0.00039465687586925924\n",
      "377 0.0003852095687761903\n",
      "378 0.000376008014427498\n",
      "379 0.00036701999488286674\n",
      "380 0.000358265737304464\n",
      "381 0.00034971264540217817\n",
      "382 0.00034147134283557534\n",
      "383 0.0003334643261041492\n",
      "384 0.0003256536729168147\n",
      "385 0.00031803635647520423\n",
      "386 0.00031060303444974124\n",
      "387 0.0003033395914826542\n",
      "388 0.0002962644211947918\n",
      "389 0.00028934323927387595\n",
      "390 0.00028260197723284364\n",
      "391 0.00027600766043178737\n",
      "392 0.0002695761213544756\n",
      "393 0.0002633026451803744\n",
      "394 0.000257178588071838\n",
      "395 0.00025119277415797114\n",
      "396 0.0002453530905768275\n",
      "397 0.00023965175205376\n",
      "398 0.000234093822655268\n",
      "399 0.00022865517530590296\n",
      "400 0.00022335100220516324\n",
      "401 0.00021817360538989305\n",
      "402 0.00021311480668373406\n",
      "403 0.0002081786806229502\n",
      "404 0.00020336004672572017\n",
      "405 0.00019865385547745973\n",
      "406 0.00019405408238526434\n",
      "407 0.00018957097199745476\n",
      "408 0.0001851889246609062\n",
      "409 0.00018091767560690641\n",
      "410 0.0001767424982972443\n",
      "411 0.00017266342183575034\n",
      "412 0.00016867888916749507\n",
      "413 0.00016479130135849118\n",
      "414 0.00016099265485536307\n",
      "415 0.0001572789333295077\n",
      "416 0.00015365392027888447\n",
      "417 0.00015011659706942737\n",
      "418 0.00014666261267848313\n",
      "419 0.0001432864519301802\n",
      "420 0.0001399929606122896\n",
      "421 0.00013676966773346066\n",
      "422 0.00013362325262278318\n",
      "423 0.00013055528688710183\n",
      "424 0.0001275532558793202\n",
      "425 0.00012462517770472914\n",
      "426 0.00012176130985608324\n",
      "427 0.00011896972137037665\n",
      "428 0.00011624239414231852\n",
      "429 0.00011357451148796827\n",
      "430 0.00011097080277977511\n",
      "431 0.00010842809570021927\n",
      "432 0.00010594378545647487\n",
      "433 0.00010351513628847897\n",
      "434 0.00010114324686583132\n",
      "435 9.88276005955413e-05\n",
      "436 9.656399197410792e-05\n",
      "437 9.435450920136645e-05\n",
      "438 9.219524508807808e-05\n",
      "439 9.008876804728061e-05\n",
      "440 8.802763477433473e-05\n",
      "441 8.601747686043382e-05\n",
      "442 8.404925756622106e-05\n",
      "443 8.212773536797613e-05\n",
      "444 8.025143324630335e-05\n",
      "445 7.841859041946009e-05\n",
      "446 7.66310840845108e-05\n",
      "447 7.48774255043827e-05\n",
      "448 7.31707041268237e-05\n",
      "449 7.150212331907824e-05\n",
      "450 6.987011147430167e-05\n",
      "451 6.827705510659143e-05\n",
      "452 6.672041490674019e-05\n",
      "453 6.519995076814666e-05\n",
      "454 6.371416384354234e-05\n",
      "455 6.226015102583915e-05\n",
      "456 6.084269261918962e-05\n",
      "457 5.945562952547334e-05\n",
      "458 5.810260699945502e-05\n",
      "459 5.677777880919166e-05\n",
      "460 5.548688204726204e-05\n",
      "461 5.422571848612279e-05\n",
      "462 5.298915129969828e-05\n",
      "463 5.178389619686641e-05\n",
      "464 5.0604361604200676e-05\n",
      "465 4.945580076309852e-05\n",
      "466 4.8330362915294245e-05\n",
      "467 4.723286838270724e-05\n",
      "468 4.615785292116925e-05\n",
      "469 4.5109954953659326e-05\n",
      "470 4.408453969517723e-05\n",
      "471 4.3082571210106835e-05\n",
      "472 4.2103714804397896e-05\n",
      "473 4.114866896998137e-05\n",
      "474 4.021421409561299e-05\n",
      "475 3.9303300582105294e-05\n",
      "476 3.8412545109167695e-05\n",
      "477 3.7538564356509596e-05\n",
      "478 3.668714634841308e-05\n",
      "479 3.585507511161268e-05\n",
      "480 3.504217602312565e-05\n",
      "481 3.42467101290822e-05\n",
      "482 3.347259189467877e-05\n",
      "483 3.2713352993596345e-05\n",
      "484 3.197096521034837e-05\n",
      "485 3.124599970760755e-05\n",
      "486 3.0537699785782024e-05\n",
      "487 2.984513048431836e-05\n",
      "488 2.9169488698244095e-05\n",
      "489 2.850770397344604e-05\n",
      "490 2.786320328596048e-05\n",
      "491 2.723089164646808e-05\n",
      "492 2.6615742171998136e-05\n",
      "493 2.601253981993068e-05\n",
      "494 2.542323636589572e-05\n",
      "495 2.4846945962053724e-05\n",
      "496 2.4284288883791305e-05\n",
      "497 2.3735687136650085e-05\n",
      "498 2.319872146472335e-05\n",
      "499 2.2673217245028354e-05\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b2ea85e-ab32-4c28-9af0-5161cfe36feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 8)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNN()\n",
    "input_data = torch.randn(1, 5)\n",
    "\n",
    "output = model(input_data)  # This calls forward internally"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
