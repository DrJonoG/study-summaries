{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94789ec-5081-4692-bde2-aa5869467b9c",
   "metadata": {},
   "source": [
    "# Module 7 - Advanced Topics in Performance Evaluation\n",
    "\n",
    "## Module Overview\n",
    "\n",
    "- Final module in the programmes first phase that covers machine learning foundations.\n",
    "\n",
    "## Learning outcomes\n",
    "\n",
    "- LO 1: Identify examples where oversampling might be useful.\n",
    "- LO 2: Apply oversampling to classification problems where the binary data of interest is rare.\n",
    "- LO 3: Estimate the performance of a given predictor using the k-fold cross-validation algorithm.\n",
    "- LO 4: Apply predictive techniques to a multi-variable, real-world data set.\n",
    "\n",
    "## Misc and Keywords\n",
    "\n",
    "## Introduction to Module Seven\n",
    "\n",
    "- Explores **two techniques for fine-tuning** the machine learning algorithms, These are\n",
    "    - **Oversampling** is used for fine tuning type one and two errors (also known as false positives, and false negatives)\n",
    "        - False positives: Incorrectly predicts a no sample is a yes sample\n",
    "        - False negatives: Incorrectly predicts a yes sample is a no sample \n",
    "    - **Cross-validation** is a more efficient approach to the train/validation and test set approach\n",
    "        - Intelligently reuses training and validation data to make data\n",
    "        - Requires more computational power\n",
    "\n",
    "## Oversampling\n",
    "\n",
    "- A technique used to address the issue of class imbalance in a dataset. Class imbalance occurs when one or more classes have significantly fewer samples than others, which can lead to biased models that favour the majority class.\n",
    "- Involves increasing the size of the minority class by adding more samples, so its representation becomes comparable to that of the majority class.\n",
    "- **Approaches**\n",
    "    - **Basic duplication**: create identical duplicates of the samples\n",
    "    - **Synthetic data** generates artifical data based on the few samples that exist, common techniques include:\n",
    "        - **SMOTE (Synthetic Minority Oversampling Technique)** generates new samples by interpolating between existing minority class samples. For example, it takes two samples from the minority class and creates a synthetic point along the line connecting them in feature space.\n",
    "        - **ADASYN (Adaptive Synthetic Sampling)** improves on SMOTE by generating synthetic samples in regions where the minority class is underrepresented or more difficult to learn.\n",
    "        - Variational Autoencoders (VAEs) or **GANs (Generative Adversarial Networks)** create realistic synthetic data points for the minority class, often used in domains like image and text data.\n",
    "    - **Weighted oversampling**: Sometimes, oversampling is combined with assigning different weights to samples to emphasise minority class data without creating excessive duplication or overfitting\n",
    "- **Challenges**\n",
    "    - Overfitting: If oversampling is performed by duplicating minority class samples, it can lead to overfitting, as the model sees the same data repeatedly.\n",
    "    - Computational Cost: Oversampling increases the size of the dataset, which may lead to higher computational requirements during training.\n",
    "    - Synthetic Sample Quality: In cases where synthetic data is generated (e.g., SMOTE), the quality of these samples can significantly impact the model's performance.\n",
    "- **Stratified sampling**  involves dividing a dataset into distinct, non-overlapping groups, known as strata, and then randomly sampling from each of these strata. The goal of stratified sampling is to ensure that each subgroup of the population is well represented in the sample, which can be particularly useful when dealing with imbalanced datasets.\n",
    "    - When you split your data into a training set and a test set, using stratified sampling ensures that the proportion of each class in both the training and test sets is the same as in the original dataset. This is essential to avoid issues where the model is trained on an unbalanced training set and evaluated on an unbalanced test set, which can result in biased performance metrics.\n",
    "    - Approach:\n",
    "        - Divide the available data into two sets (strata):\n",
    "            - all samples of the class of interest (The rare class) (set A).\n",
    "            - all other samples (set B).\n",
    "        - Construct the training set:\n",
    "            - randomly select 50 per cent of the samples in set A.\n",
    "            - add equally many samples from set B.\n",
    "        - Construct the validation set:\n",
    "            - select the remaining 50 per cent of samples from set A.\n",
    "            - add enough samples from set B so as to restore the original ratio from the overall data set\n",
    "\n",
    "## K-Fold Cross-validation\n",
    "- Splitting data:\n",
    "    - Since you need to split the data into different sets, you almost invariably end up with either too little training data or too little validation data.\n",
    "    - If the data is randomly split into training and validation data, the approach gives different results whenever the data is reshuffled.\n",
    "- Cross-validation can alleviate both of the shortcomings above.\n",
    "- The K-folds process\n",
    "    - Split the data into equal parts, k (which is often 5, 7 or 10)\n",
    "    - For each iteration i = {1, 2..k} select i as the validation set, and the others as training\n",
    "    - After k iterations average the validation performance over all k runs\n",
    "    - Select the model with the best average validation performance\n",
    "- For example:\n",
    "    - $\\text{The dataset} = [x_1 = 3.2, x_2 = 1.7, x_3 = 7.2, x_4 = 4.0, x_5 = 8.1]$ \n",
    "    - For the fold above, the average predictor is (1.7 + 7.2 + 4.0 + 8.1) / 4 = 5.25, and its mean absolute error on the validation set is | 3.2 â€“ 5.25 | = 2.05.\n",
    "    - Similarly, for the other folds, we obtain:\n",
    "        - Fold 2: average predictor 5.625, mean absolute error on validation set 3.925\n",
    "        - Fold 3: average predictor 4.25, mean absolute error on validation set 2.95\n",
    "        - Fold 4: average predictor 5.05, mean absolute error on validation set 1.05\n",
    "        - Fold 5: average predictor 4.025, mean absolute error on validation set 4.075\n",
    "    - So, the overall estimate of the mean absolute error is (2.05 + 3.925 + 2.95 + 1.05 + 4.075) / 5 = 2.81. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BaseGPU",
   "language": "python",
   "name": "basegpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
