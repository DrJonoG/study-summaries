{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94789ec-5081-4692-bde2-aa5869467b9c",
   "metadata": {},
   "source": [
    "# Module 10 - Decision Trees Part Two\n",
    "\n",
    "---\n",
    "\n",
    "## Learning outcomes\n",
    "\n",
    "- LO 1: Select the most appropriate tree depth for making a prediction.\n",
    "- LO 2: Identify important elements of tree ensembles.\n",
    "- LO 3: Implement important elements of tree ensembles.\n",
    "- LO 4: Recognise the importance of features in tree ensembles.\n",
    "- LO 5: Identify real-life applications of decision trees.\n",
    "- LO 6: Discuss the concepts of interpretability and fairness\n",
    "- LO 7: Compare the functionality of k-nearest neighbours and decision trees.\n",
    "\n",
    "## Misc and Keywords\n",
    "- **Perturbing** is the process of adding noise to data\n",
    "- **Interpretability** in machine learning is often described as whether a human can understand why the machine learning model has made the decision it has. Interpretability can be an important attribute as it helps us understand if the model is working correctly and fairly. Not all models are interpretable, and some non-interpretable models are very useful. However, people may be more likely to adopt an interpretable model in certain contexts. For example, doctors are much more likely to use a machine learning model if they understand how it’s coming to decisions compared to a black box model where the decision-making process is more opaque.\n",
    "- **Fairness** in AI is the concept that any AI model should treat all people equally and fairly. It is important that machine learning practitioners don’t allow prejudices and biases from society to creep into their models and reinforce inequalities that marginalised groups already face. Models become biased when the people building the algorithms may deliberately or inadvertently include their own prejudices in how they build the model. This is one reason why it is important to have fair representation among machine learning practitioners and for practitioners to consult affected populations (e.g. patients or customers) when building a model.\n",
    "\n",
    "## Module Summary Description\n",
    "- Looks at an approach to avoid overfitting which is more powerful than tree pruning. This is known as **tree ensembles**\n",
    "- How do tree ensembles work:\n",
    "    - Each individual tree is unique which is achieved by perturbing (adding noise) the training data i.e., using boot strapping\n",
    "    - Each tree will be more robust against noise\n",
    "    - For any new data point, the output is predicted either through majority vote (classification) or averaging (regression)\n",
    "- When using tree ensembles the disadvantage is the loss of interpretability. That is we can understand what is going on and how the choices are made when the trees are individual.\n",
    "- When deciding whether to use ensembles or individual trees you need to decide which is more important, the predictve performance (ensembles) or the interpretability (individual)\n",
    "- Three ways to construct tree ensembles: Bagging, Random Forests, AdaBoost\n",
    "- Tree ensembeles reduce variance at the cost of a higher bias since optimal splits are not taken in each tree node\n",
    "\n",
    "---\n",
    "\n",
    "## Selecting the Tree Depth\n",
    "- If the tree depth is too shallow, your model won’t be as accurate, and you’ll get a high misclassification rate.\n",
    "- If the tree depth is too deep, your model will overfit to the training data and won’t perform well when presented with new data.\n",
    "\n",
    "## Bagging\n",
    "- Decision trees are very powerful but can suffer from a high variance, one solution is to go from a single decision tree to a collection of decision trees. This involves combining bootstrapping with averaging which is known as the **bagging algorithm**\n",
    "- Bagging works as follows:\n",
    "    - **Tree construction**: For each decision tree $b = 1, ... B$, i.e., $(B = 100)$\n",
    "        - Sample $n$ training data points with replacement (bootstrapping)\n",
    "        - Grow a complete tree from these samples\n",
    "    - **Prediction**\n",
    "        - Predict outcome of a new datapoint in each tree $b = 1, ..., B$\n",
    "        - Take a majority vote (classification) or average (regression)\n",
    "- By using bagging it is expected that the overall variance of our prediction will have decreased, however this is not always the case.\n",
    "    - This occurs when the samples are highly correlated i.e. a high correlation coefficient\n",
    " \n",
    "### Decorrelating Trees\n",
    "- Two algorithms to decorrelate trees, **random forests** and **boosting**\n",
    "\n",
    "##### Random Forests\n",
    "- The idea is to force the decision trees to look at different subsets of predictors in each level\n",
    "- **Tree construction**: For each decision tree $b = 1, ... B$, i.e., $(B = 100)$\n",
    "    - Sample $n$ training data points with replacement (bootstrapping)\n",
    "    - Grow a complete tree from these samples\n",
    "    - **however** in each splitting step only consider a random sample of $m$ predictors, (i.e., $m =\\sqrt{p}$ where $\\text{p = number of precitors}$). That is instead or using all of $n$ we use a sample of them.\n",
    "- **Prediction**\n",
    "    - Predict outcome of a new datapoint in each tree $b = 1, ..., B$\n",
    "    - Take a majority vote (classification) or average (regression)\n",
    "- Consquently, random forests introduces a bias, but in the hope that bias is far oughtweighed by the reduction in variance.\n",
    "\n",
    "##### Boosting\n",
    "- The key idea is to attach a weight to each training sample, the higher the weight, the more important the training sample\n",
    "- Initialise by giving all training data records $i = 1, ..., N$ the same weight $w_i$\n",
    "- **Tree construction**, For each decision tree $b = 1, ... B$, i.e., $(B = 1,000)$\n",
    "    - Grow a shallow tree (i.e, depth 1 or 2) from weighted records\n",
    "    - Increase the weights of the misclassified training records\n",
    "    - Decrease the weights of the correctly classified records, and repeat for each tree.\n",
    "- **Prediction**\n",
    "    - Predict outcome of a new datapoint in each tree $b = 1, ..., B$\n",
    "    - Take a majority vote (classification) or average (regression)\n",
    "    - For better performance, weight the contribution of the trees by their misclassification rate\n",
    "\n",
    "#### Tree Ensembles\n",
    "- In the assignment 10.2 for tree ensembles, we:\n",
    "    - Evaluate three models;\n",
    "        - A single decision tree\n",
    "        - A random forest classifier\n",
    "        - The AdaBoost classifier\n",
    "    - We select the best model, that is the one that performs best on validation data\n",
    "    - We can then use feature select to help understand the model and remove any irrelevant features. Two approaches in particular were:\n",
    "        - Impurity metric approach\n",
    "            - One of the downsides of the inbuilt impurity metric is that it can only be applied to training data. This doesn't give us any indication of which features will be the most important on unseen data.\n",
    "            - The Impurity Metric approach evaluates feature importance based on how much a feature reduces the impurity in decision trees. Impurity measures how mixed the target variable is within the subsets created by a decision tree’s splits. Common impurity measures include Gini Impurity or Entropy. When a feature is used in a decision tree split, it divides the data into purer subsets—subsets with more homogenous target values. The more a feature reduces impurity, the more important it is for the model's decision-making process. Features that result in larger reductions in impurity are deemed more important, while features with minimal impact on impurity reduction are considered less important. This method inherently works well with decision trees, as it directly measures the contribution of each feature to the tree's ability to make accurate splits and predictions.\n",
    "        - Permutation importances\n",
    "            - we can use the permutation importance to measure the feature importances on both the training and validation sets.\n",
    "            - The permutation importance function measures the change in model performance when the feature values are randomly shuffled. If a feature is shuffled and has minimal impact on the model's performance, it means the feature has little importance, and its permutation importance score will be close to zero. On the other hand, if shuffling a feature actually improves the model's performance, this suggests that the feature might be harmful or irrelevant to the model. In this case, the feature is assigned a negative permutation importance, indicating that it could introduce noise or cause overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8730668-4a6c-4812-b489-fefa12cb76cf",
   "metadata": {},
   "source": [
    "### Determining the importance of features in tree ensembles\n",
    "- Often, you want to find out which features in your decision tree are the most important, meaning which features have a larger effect on the predicted outcome than others. This helps you to interpret the results and allows you to drop features that turn out to be unimportant. You can determine the relative importance of factors using a feature importance metric.\n",
    "- One way to think about a feature’s importance is through the expected fraction of samples the feature will contribute to and the reduction in impurity you get from splitting on that variable. Many packages have inbuilt feature selection metrics which weigh the reduction in impurity (i.e. Gini index or entropy) with the number of samples that split affects. It’s also easy to average this metric over many trees to get the mean decrease in impurity.\n",
    "- However, these impurity-based metrics have some drawbacks:\n",
    "    - Impurity-based metrics tend to favour features with many unique values. This means they will often put a lot more importance on numerical features than on categorical ones with few categories.\n",
    "    - Training data is used to calculate impurity-based metrics, meaning they don’t give any indication of performance on held-out test data.\n",
    "- An alternative measure of feature importance is the permutation importance. This works by calculating the increase in the model’s predictive error after randomly permuting a variable. This is similar to randomly shuffling one column of the input data and seeing how that would affect the predictive error of the model. The permutation importance is a useful metric, as it is quick to calculate, easy to interpret and usable on training and test data. However, the random nature of the permutation means the metric may give slightly different answers depending on the random seed used.\n",
    "- One other thing to be aware of when using the permutation importance is that if we have two highly correlated features, the feature importance of both will lessen. For example, say you are predicting the number of visitors to a park and, out of the features you are looking at, temperature is the most important one. The permutation importance would rank the temperature highest. If you were then to add a time-of-year feature, which is highly correlated with temperature, the importance of both the time of year and temperature would drop, meaning they may rank somewhere in the middle of the feature importance list.\n",
    "- Feature selection can help us understand our model and the outputs it gives us, as well as remove any irrelevant predictors. In this section, we will be looking at how to identify the most important features in a decision tree using two different methods\n",
    "    - Impurity metric approaches\n",
    "    - Permutation importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8f9a32-9c28-40fc-bb54-7c18627398c6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Random Forests vs AdaBoost\n",
    "\n",
    "### **Random Forests**\n",
    "- **Type**: Bagging (Bootstrap Aggregating)  \n",
    "- **Base Learner**: Decision Trees (typically deep trees)  \n",
    "- **Training**: Trains multiple decision trees independently on random subsets of the data with replacement (bootstrapping).  \n",
    "- **Combination**: Averages (for regression) or majority vote (for classification) from all trees.  \n",
    "- **Strengths**:\n",
    "    - Reduces overfitting compared to individual decision trees.  \n",
    "    - Works well with high-dimensional data and missing values.  \n",
    "    - Robust to noisy data.  \n",
    "- **Weaknesses**:\n",
    "    - Can be computationally expensive.  \n",
    "    - Predictions are not as interpretable as a single decision tree.  \n",
    "\n",
    "### **AdaBoost (Adaptive Boosting)**\n",
    "- **Type**: Boosting  \n",
    "- **Base Learner**: Weak learners (often shallow decision trees called \"stumps\")  \n",
    "- **Training**: Sequentially trains weak models, where each subsequent model focuses more on misclassified instances from the previous ones by assigning higher weights to difficult samples.  \n",
    "- **Combination**: Weighted sum of weak learners' predictions.  \n",
    "- **Strengths**:\n",
    "    - Often achieves higher accuracy than Random Forests on clean and structured data.  \n",
    "    - Works well with simple models and avoids overfitting.  \n",
    "    - More interpretable than Random Forests.  \n",
    "- **Weaknesses**:\n",
    "    - Sensitive to noisy data and outliers since it gives higher weight to misclassified points.  \n",
    "    - Can be slower for large datasets due to sequential training.  \n",
    "\n",
    "### **When to Use Which?**\n",
    "- **Use Random Forests** when you need a robust model that handles large datasets well and is resistant to noise.  \n",
    "- **Use AdaBoost** when you want a model that prioritises difficult cases and when your dataset is relatively clean and structured.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4e2fc0-5286-41a5-aa0e-5992a1bb44b3",
   "metadata": {},
   "source": [
    "### Bootstrapping\n",
    "- Bootstrapping typically refers to sampling with replacement from the dataset to create different subsets of the original data. Whether it includes both instances and features multiple times depends on the specific method used:\n",
    "1. Bootstrapping in Bagging (Bootstrap Aggregating)\n",
    "    - In methods like Bagging (e.g., Random Forests):\n",
    "        - Bootstrapping is applied to instances (rows), meaning some instances may be selected multiple times while others may not be selected at all.\n",
    "        - However, all features (columns) are usually used for training each model unless additional feature selection is applied.\n",
    "2. Bootstrapping in Random Forests (Instance & Feature Selection)\n",
    "    - Random Forests extend bagging by adding random feature selection:\n",
    "        - Instances are sampled with replacement (bootstrapped)\n",
    "        - Features are randomly selected for each decision tree\n",
    "        - This means that both instances and features can appear multiple times in different trees, but not necessarily in the same tree.\n",
    "3. Bootstrapping in General Statistics (Resampling)\n",
    "    - In traditional bootstrapping (used in statistics to estimate confidence intervals):\n",
    "        - Only instances (observations) are resampled.\n",
    "        - Features (variables) remain the same.\n",
    "4. Bootstrapping in Gradient Boosting?\n",
    "    - Gradient Boosting (GBM, XGBoost, LightGBM) typically does not use bootstrapping by default.\n",
    "    - Instead, it builds trees sequentially and can use subsampling (without replacement) of instances or features for regularisation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BaseGPU",
   "language": "python",
   "name": "basegpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
