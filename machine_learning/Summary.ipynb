{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "096ccee1-f9cc-4de5-97c9-fe813ddcceda",
   "metadata": {},
   "source": [
    "# ICL Course Reference\n",
    "\n",
    "## Formulas\n",
    "\n",
    "#### **Binomial Distribution**\n",
    "- **Binomial Coefficient**:\n",
    "    - $\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$\n",
    "        - $n$ = the number of trials,\n",
    "        - $k$ = the number of successes.\n",
    "- **Probability Mass Function**:\n",
    "    - $f(k) = \\binom{n}{k} \\cdot \\theta^k \\cdot (1 - \\theta)^{n-k}$\n",
    "        - $n$ = the number of trials,\n",
    "        - $\\theta$ = the probability of success on a trial,\n",
    "        - $\\binom{n}{k}$ = the binomial coefficient,\n",
    "        - $k$ = the number of successes in $n$ trials.\n",
    "- **Binomial Likelihood Function**:\n",
    "    - $L(\\theta) = \\binom{n}{s} \\cdot \\theta^s \\cdot (1 - \\theta)^{n - s}$\n",
    "        - $s$ = the number of successes,\n",
    "        - $n$ = the number of trials,\n",
    "        - $\\theta$ = the probability of success.\n",
    "\n",
    "#### **Likelihood and Probability**\n",
    "- **General Likelihood Function**:\n",
    "    - $L(\\theta | X) = P(X | \\theta) = \\prod_{i=1}^{n} f(x_i | \\theta)$\n",
    "        - $X$ = observed data ($x_1, x_2, \\dots, x_n$),\n",
    "        - $\\theta$ = parameter(s) of interest,\n",
    "        - $f(x_i | \\theta)$ = probability (or density) of $x_i$ given $\\theta$.\n",
    "- **Log-Likelihood Function**:\n",
    "    - $\\ell(\\theta | X) = \\log L(\\theta | X) = \\sum_{i=1}^{n} \\log f(x_i | \\theta)$\n",
    "- **Total Probability**:\n",
    "    - $P(X) = \\sum_Y P(X, Y)$\n",
    "- **Conditional Probability**:\n",
    "    - $P(A|B) = \\frac{P(A \\cap B)}{P(B)}$\n",
    "        - $P(A|B)$ = probability of $A$ given $B$,\n",
    "        - $P(A \\cap B)$ = probability of both $A$ and $B$,\n",
    "        - $P(B)$ = probability of $B$.\n",
    "- **Probability Density Function (Normal Distribution)**:\n",
    "    - $f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$\n",
    "        - $f(x)$ = probability density at $x$,\n",
    "        - $\\mu$ = mean,\n",
    "        - $\\sigma^2$ = variance,\n",
    "        - $\\sigma$ = standard deviation.\n",
    "- **Bayes' Theorem**:\n",
    "    - $P(Y|X) = \\frac{P(X|Y) \\cdot P(Y)}{P(X)}$\n",
    "        - $P(Y|X)$ = is the posterior probability of the parameters $Y$\n",
    "        - $P(X|Y)$ = likelihood of the data $X$,\n",
    "        - $P(Y)$ = the prior probability of the parameters\n",
    "        - $P(X)$ = the marginal likelihood (or evidence).\n",
    "#### **Statistical Measures**\n",
    "- **Variance**:\n",
    "    - $\\text{Var}(X) = \\mathbb{E} \\left[ (X - \\mathbb{E}(X))^2 \\right] = \\sum_i (x_i - \\mathbb{E}(X))^2 \\cdot P(x_i)$\n",
    "        - $\\text{Var}(X)$ = variance of $X$,\n",
    "        - $\\mathbb{E}(X)$ = expected value (mean) of $X$,\n",
    "        - $x_i$ = specific possible value of $X$,\n",
    "        - $P(x_i)$ = probability of $x_i$.\n",
    "- **Covariance**:\n",
    "    - $\\text{Cov}(X, Y) = \\mathbb{E}[(X - \\mathbb{E}(X))(Y - \\mathbb{E}(Y))]$\n",
    "        - $\\text{Cov}(X, Y)$ = covariance between $X$ and $Y$,\n",
    "        - $\\mathbb{E}(X)$ = expected value of $X$,\n",
    "        - $\\mathbb{E}(Y)$ = expected value of $Y$.\n",
    "\n",
    "#### **Regression Analysis**\n",
    "- **Regression Line**:\n",
    "    - $Y = aX + b$\n",
    "        - $a$ = slope (regression coefficient),\n",
    "        - $b$ = y-intercept,\n",
    "        - $X$ = independent variable,\n",
    "        - $Y$ = dependent variable.\n",
    "- **Regression Coefficient**:\n",
    "    - $a = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}$\n",
    "        - $\\bar{x}$ = mean of $X$,\n",
    "        - $\\bar{y}$ = mean of $Y$.\n",
    "- **Y-Intercept**:\n",
    "    - $b = \\bar{y} - a\\bar{x}$\n",
    "- **Correlation Coefficient**:\n",
    "    - $r = \\frac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_i (x_i - \\bar{x})^2} \\sqrt{\\sum_i (y_i - \\bar{y})^2}}$\n",
    "        - $r$ = correlation coefficient,\n",
    "        - $\\bar{x}$ = mean of $X$,\n",
    "        - $\\bar{y}$ = mean of $Y$.\n",
    "\n",
    "\n",
    "\n",
    "#### **Generalisation Bound**\n",
    "- **Generalisation Bound**:\n",
    "    - $n \\geq \\frac{\\log\\left(\\frac{\\delta}{N-1}\\right)}{\\log(1 - \\epsilon)}$\n",
    "        - $n$ =  required number of samples,\n",
    "        - $\\delta$ = confidence level,\n",
    "        - $N$ = number of candidate functions,\n",
    "        - $\\epsilon$ = error rate of incorrect functions.\n",
    "\n",
    "#### **Performance Measures For Regression Problems**\n",
    "- **Mean Absolute Error (MAE)**  \n",
    "    - $\\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $\n",
    "        - $n$ = number of samples,\n",
    "        - $y_i$ = actual value,\n",
    "        - $\\hat{y}_i$ = predicted value.\n",
    "- **Average Error (AE)**  \n",
    "    - $\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i) $\n",
    "- **Mean Absolute Percentage Error (MAPE)**    \n",
    "    - $  \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| \\times 100 $\n",
    "- **Root Mean Squared Error (RMSE)**  \n",
    "    - $\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $\n",
    "- **Total Sum of Squared Errors (SSE)**  \n",
    "    - $  \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $\n",
    "#### Metrics Derived from the Confusion Matrix:\n",
    "- **Accuracy**:  \n",
    "   - $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "       - $TP$ = Correct positive predictions.\n",
    "       - $TN$ = Correct negative predictions.\n",
    "       - $FP$ = Incorrect positive predictions (Type I error).\n",
    "       - $FN$ = Incorrect negative predictions (Type II error).\n",
    "- **Precision**:\n",
    "   - $\\frac{TP}{TP + FP}$\n",
    "- **Recall (Sensitivity)**:  \n",
    "   - $\\frac{TP}{TP + FN}$\n",
    "- **F1-Score**:  \n",
    "   - $2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n",
    "- **Specificity**:\n",
    "   - $\\frac{TN}{FP + TN}$\n",
    "- **Total Error Rate**\n",
    "   - $\\frac{FN + FP}{TP + FN + FP + TN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6478f2f6-7ae0-42f7-8ece-a59d9d1d0a37",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "\n",
    "- **Stochastic Data Models**: These models treat the data generation process as a random variable. Common examples include normal, binomial, and Student’s t-distributions.\n",
    "- **Goodness of Fit**: Summarizes the discrepancy between observed and expected values. A common measure is the root mean squared error (RMSE), which quantifies the differences between the predicted and actual values.\n",
    "- **Predictive Accuracy**: Refers to how well the predicted values match the actual observed values.\n",
    "- **Generalization**: The ability of a model to perform well on new, unseen data, rather than just fitting the data it was trained on.\n",
    "- **Summary Statistics**: A set of values that describe the central tendency, spread, and shape of a dataset, such as mean, standard deviation, and median.\n",
    "- **Black Box**: A system or process where the internal workings are hidden or not fully understood, often used to describe complex models or algorithms.\n",
    "- **Model Validation**: The process of confirming that a model is performing as expected and meets its intended purpose, often using validation data or cross-validation.\n",
    "- **Residual Analysis**: The residual is the difference between the observed and predicted values. Analyzing these residuals can help assess the accuracy of a model and identify patterns or systematic errors.\n",
    "- **Deterministic Input**: An input that produces the same output every time it is used, often derived from historical data, standards, or specifications.\n",
    "- **Linear Regression**: A method for modeling the relationship between two or more variables. In simple linear regression, the goal is to fit a straight line, while in multiple linear regression, a hyperplane is used to best represent the relationship.\n",
    "- **Logistic Regression**: Used for binary classification, where the goal is to predict one of two possible outcomes. Unlike linear regression, which predicts continuous values, logistic regression predicts probabilities that can be mapped to discrete classes.\n",
    "- **Stochastic Events**: Events that involve randomness or uncertainty, where the outcomes cannot be precisely predicted.\n",
    "- **Poisson Distribution**: A probability distribution that models the number of events occurring within a fixed time period or space, often used in scenarios like modeling the number of customer arrivals or system failures over time.\n",
    "- **Statistics**: Involves analyzing data to make inferences about a population or process. It involves fitting a model that explains the observed data (Data → Model).\n",
    "- **Probability**: Describes what we expect to happen in a random experiment. It helps us understand the data or the properties of the data that the model can produce (Model → Data).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292a0bfd-639a-4ddb-b0b9-96e25705074a",
   "metadata": {},
   "source": [
    "## **Machine Learning (ML)**\n",
    "- **Machine Learning (ML)** is about understanding the relationship between input variables (also called features or predictors) and an output. This relationship can be described by the equation: \n",
    "  $$ Y = f(X_1, X_2, ..., X_n) + \\alpha $$\n",
    "  where $Y$ is the output, $X_1, X_2, ..., X_n$ are the input features, and $\\alpha$ is a constant (often a bias term).\n",
    "- In **supervised learning**, both $X$ (input variables) and $Y$ (output or target) are provided during training. In **unsupervised learning**, only the input data $X$ is provided, and the algorithm seeks to learn the structure or patterns in the data without predefined output labels.\n",
    "- ML models typically serve two main purposes:\n",
    "    - **Forecasting**: Predicting future events (denoted as $\\hat{y}$) based on historical data.\n",
    "    - **Inference**: Understanding the relationship between $Y$ and $X$. For example, understanding why certain products sell better than others.\n",
    "        - **Statistical inference** involves drawing conclusions about a population based on sample data.\n",
    "        - **Model inference** refers to using a trained model to make predictions on new, unseen data.\n",
    "- Two main types of supervised learning:\n",
    "    - **Prediction** (also called **regression**) estimates a continuous numerical value, e.g., predicting how much or how many of something.\n",
    "    - **Classification** estimates a discrete class or category, e.g., determining whether an email is spam or not.\n",
    "- Two main categories of models:\n",
    "    - **Parametric** models make strong assumptions about the data's underlying structure. For example, assuming a linear relationship between size and price in a real-estate dataset (i.e., \"There's a straight-line relationship between size and price\").\n",
    "    - **Non-parametric** models make fewer assumptions about the structure of the data and instead learn directly from the data itself. For example, using the prices of similar houses to predict the price of a new house without assuming a specific mathematical form.\n",
    "- **Imputation** refers to methods for dealing with missing data. Some common techniques for handling missing data include:\n",
    "    - **Removal**: Deleting the data entries that have missing values.\n",
    "    - **Simple imputation**: Substituting missing values with statistical measures such as the mean or median of the column.\n",
    "    - **Normalization**: Normalizing the values to estimate the missing value.\n",
    "    - **Regression imputation**: Using regression models to predict missing values based on other variables in the dataset.\n",
    "    - **Random regression imputation**: Adding a random component to the regular regression imputation. After predicting the missing value using regression, a residual term is added to introduce randomness.\n",
    "    - **Multiple imputation**: Using all available features in the data to forecast and fill in missing values, often involving multiple imputed datasets to account for uncertainty in the imputations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a2e5d5-e279-401c-a9da-13f6755ce612",
   "metadata": {},
   "source": [
    "## **Probability**\n",
    "- **Probability distribution**: Describes how likely different possible outcomes are to occur in a random event or experiment. There are two main types:\n",
    "    - **Discrete Probability Distributions** deal with outcomes that can only take specific, countable values (e.g., flipping a coin or rolling a die).\n",
    "    - **Continuous Probability Distributions** deal with outcomes that can take any value within a continuous range (e.g., measuring a person's height or time).\n",
    "- A **stochastic event** is any event that involves randomness or uncertainty in its outcome. For example, each time you roll a die, you can't predict the exact outcome, but you can describe the probability of each possible result.\n",
    "    - **Discrete Stochastic Events** take specific countable values, typically whole numbers or categories. These events use **Probability Mass Functions (PMF)**.\n",
    "    - **Continuous Stochastic Events** take any value within a continuous range. These events use **Probability Density Functions (PDF)**.\n",
    "- A **Probability Mass Function (PMF)** is used for discrete random variables. It can be represented as a bar chart, where the height of each bar represents the exact probability of a specific outcome.\n",
    "- A **Probability Distribution Function (PDF)** is used for continuous random variables. It is represented as a smooth curve, where the probability is found by calculating the area under the curve between two points. It describes the likelihood of the random variable $x$ taking on a particular value within a given range.\n",
    "- The **binomial distribution** is a probability distribution that describes the number of successes in a fixed number of independent trials, where each trial has two possible outcomes: success or failure. For example, flipping a fair coin multiple times and counting the number of heads. This distribution is widely used in machine learning for modeling binary outcomes.\n",
    "- **Binomial coefficients** represent the number of ways to choose $k$ successes from $n$ trials. It is mathematically expressed as: \n",
    "    $$ \\binom{n}{k} = \\frac{n!}{k!(n - k)!} $$\n",
    "- The **Central Limit Theorem (CLT)** explains how the distribution of the sample mean (or sum) of a large number of independent, identically distributed random variables will approach a normal distribution (also called a Gaussian distribution), regardless of the original distribution of the data. The key aspects of CLT involve:\n",
    "    - **Grouping events**: Taking multiple observations (e.g., dice rolls, test scores, etc.) and dividing them into groups.\n",
    "    - **Calculating averages**: For each group, compute the average of the observations within that group.\n",
    "    - **Plotting the averages**: When these averages are plotted as a distribution, the shape tends to resemble a bell curve (Normal Distribution), even if the original data (individual events) is not normally distributed.\n",
    "- **Absolute Probability** (also called **Unconditional Probability**) refers to the probability of an event occurring without any prior conditions or restrictions. It is the most straightforward form of probability and does not depend on any other events. For example, the probability of rolling a 4 on a fair six-sided die is:\n",
    "    $$ P(4) = \\frac{1}{6} $$\n",
    "- **Conditional Probability** is the probability of an event occurring, given that another event has already occurred. It reflects the relationship between two events and is denoted as $P(A|B)$, meaning **the probability of A given B**.\n",
    "- **Total Probability** refers to the overall probability of an event occurring, accounting for all possible scenarios or conditions. It is calculated using the **law of total probability**, which involves partitioning the sample space into mutually exclusive events $B_1, B_2, \\dots, B_n$ such that:\n",
    "    $$ P(A) = \\sum_{i=1}^{n} P(A|B_i) P(B_i) $$\n",
    "- **Independent events** occur when the outcome of one event does not affect the outcome of another event. For independent events, the probability of their co-occurrence (composite event) is the product of their individual probabilities:\n",
    "    $$ P(X, Y) = P(X) P(Y) = P(X = x) P(Y = y) $$\n",
    "- A **complementary event** in probability refers to all the outcomes in the sample space that are not part of the given event. It is essentially the opposite of the event you're considering. If $A$ is an event, then the complement of $A$, denoted as $A^c$, is the event that $A$ does not occur. The probability of the complement of an event is given by:\n",
    "    $$ P(A^c) = 1 - P(A) $$\n",
    "- **Bayes' Theorem** (or Bayes' Rule) describes how to update the probability of a hypothesis (or event) based on new evidence. It is mathematically represented as:\n",
    "    $$ P(Y|X) = \\frac{P(X|Y) \\cdot P(Y)}{P(X)} $$\n",
    "    - Where:\n",
    "        - $P(Y|X)$ is the **posterior probability** (the probability of $Y$ given $X$).\n",
    "        - $P(X|Y)$ is the **likelihood** (the probability of observing $X$ given $Y$).\n",
    "        - $P(Y)$ is the **prior probability** (the initial probability of $Y$).\n",
    "        - $P(X)$ is the **evidence** (the probability of observing $X$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad17db76-0c6b-4cd8-a2d9-856663ae464d",
   "metadata": {},
   "source": [
    "## Descriptive Statistics\n",
    "\n",
    "- **1. Measures of Central Tendancy**\n",
    "    -  These describe the center, or typical value, of a dataset\n",
    "    -  **Mean**\n",
    "        - The average of the data\n",
    "        - $\\mu = \\frac{\\sum{x_i}}{N}$ \n",
    "    -  **Median**\n",
    "        - The middle value when data is ordered\n",
    "    -  **Mode**\n",
    "        - The most frequently occuring value     \n",
    "    -  If the mean < median then data is left skewed, and implies some low outliers. And vice versa.\n",
    "- **2. Measures of Dispersion**\n",
    "    - These describe the variability or spread of a dataset\n",
    "    - **Range**\n",
    "        - $\\text{range} = max(x) - min(x)$\n",
    "    - **Variance**\n",
    "        - The average squared deviation from the mean ($\\mu$)\n",
    "        - $\\sigma^2 = \\frac{\\sum{(x_i-\\mu)^2}}{N} \\text{(population)}$\n",
    "        - $\\sigma^2 = \\frac{\\sum{(x_i-\\mu)^2}}{N-1} \\text{(sample)}$\n",
    "    - **Standard Deviation**\n",
    "        - The square root of the variance, providing a measure in the same units as the data\n",
    "        - $\\text{standard deviation} = \\sqrt{\\sigma^2}$\n",
    "    - **Coefficient of Variation (CV)**\n",
    "        - The ratio of the standard deviation to the mean as a percent\n",
    "        - $\\text{CV} = \\frac{Standard Deviation}{\\mu}\\cdot 100$\n",
    "            - CV < 1: Lower relative variability\n",
    "            - CV ≈ 1: High relative variability\n",
    "            - CV > 1: Very high relative variability\n",
    "- **3. Measure of Shape**\n",
    "    - Used to describe the shape and symmetry of the dataset \n",
    "    - **Skewness**\n",
    "        - $\\text{skewness} = \\frac{\\frac{1}{N}\\sum{(x_i - \\mu)}}{\\sigma^3}$\n",
    "        - Measures the asymmetry of the distribution.\n",
    "        - skewness > 0 = right skewed\n",
    "        - skewness < 0 = left skewed\n",
    "        - skeness = 0 = symmetric\n",
    "    - **Modaility**\n",
    "        - The number of peaks\n",
    "        - **Unimodal** One peak\n",
    "        - **Bimodal** Two peak\n",
    "        - **Multimodal** More than two peaks\n",
    "- **4. Measure of Position**\n",
    "    - **Percentiles**\n",
    "        - Values divide into 100 equal parts, the $k-th$ percentile is the value below the $k\\%$ of the data falls\n",
    "    - **Quartiles**\n",
    "        - Special percentiles dividing data into four equal parts:\n",
    "            - $Q_1$ = (25th percentile): Lower quartile.\n",
    "            - $Q_2$ = (50th percentile): Median.\n",
    "            - $Q_3$ = (75th percentile): Upper quartile.\n",
    "    - **Deciles**\n",
    "        - Divide data into 10 equal parts\n",
    "    - **Z-Scores**\n",
    "        - Measure how many standard deviations a data point is from the mean\n",
    "        - $z = \\frac{x - \\bar{x}}{\\text{standard deviation}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0689efdf-14d9-4b1d-8754-3f111e524eb8",
   "metadata": {},
   "source": [
    "## **Misc Statistics**\n",
    "\n",
    "- **Statistical inference** is the process of using data from a sample to make conclusions or predictions about a larger population. Frequentist and Bayesian are two distinct paradigms of reasoning and methodology in statistical inference.\n",
    "    - **Frequentist**: Views probability as the long-run frequency of events; parameters are fixed, and data is used to estimate them (e.g., hypothesis testing, confidence intervals). Frequentists believe that the \"true\" value of a statistic about a population (for example, the mean) is fixed and unknown.\n",
    "    - **Bayesian**: Treats probability as a degree of belief; parameters are treated as random variables and updated using prior knowledge and observed data (via Bayes' Theorem). Bayesians believe that data inform us about the distribution of a statistic or event, and that as we receive more data, our belief about the distribution can be updated, confirming or revising our previous beliefs. \n",
    "- **Outliers** are data points that significantly differ from the rest of the data. Methods to detect and remove outliers include:\n",
    "    - **Z-Score**: Identifies outliers based on how many standard deviations away a point is from the mean.\n",
    "    - **Modified Z-Score**: Uses the median and MAD (Median Absolute Deviation) for small datasets or non-normal distributions.\n",
    "    - **Tukey's Fences**: An aggressive IQR-based method with different \"fence\" thresholds to identify outliers.\n",
    "    - **Percentile Approach**: Identifies outliers by examining the distribution of data using percentiles, which divide the data into 100 equal parts.\n",
    "    - **Quartile Approach (IQR)**: Identifies outliers based on the Interquartile Range (IQR), dividing the data into four equal parts using quartiles:\n",
    "        - Outliers are typically considered as those data points outside the range: \n",
    "        $$ Q_1 - 1.5 \\cdot \\text{IQR} \\quad \\text{or} \\quad Q_3 + 1.5 \\cdot \\text{IQR} $$\n",
    "    - **Visualization**: Boxplots and scatter plots help visually identify outliers.\n",
    "    - **Isolation Forest**: A machine learning method that isolates outliers by partitioning the data and determining how isolated each point is.\n",
    "    - **DBSCAN**: A clustering-based method that detects outliers as points that do not belong to any cluster.\n",
    "    - **Anomaly Detection**: Advanced machine learning techniques like One-Class SVM or Autoencoders for detecting outliers.\n",
    "- **Variance** measures the spread of a single variable around its mean, calculated as the average of squared deviations from the mean:\n",
    "    $$ \\text{Variance of } X = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2 $$\n",
    "- **Covariance** measures the relationship between two variables and how they vary together. It is calculated as:\n",
    "    $$ \\text{Covariance of } X \\text{ and } Y = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) $$ \n",
    "- **Bootstrapping** is a resampling technique used to assess the accuracy of a statistic (e.g., correlation coefficient, mean, median) by repeatedly sampling from the original dataset. It involves generating many new datasets (called \"bootstrap samples\") by randomly sampling data points from the original dataset with replacement (i.e., some data points may appear more than once in a new sample, while others may not appear at all).\n",
    "    - It is a **non-parametric method**, meaning it does not rely on assumptions about the distribution of the original data.\n",
    "    - Bootstrapping is often used to estimate the sampling distribution of a statistic, calculate confidence intervals, and assess the stability or variability of a model's estimates (including correlation coefficients between two time series, regression coefficients, etc.).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17625211-27a7-48af-aabd-3afaf0e45824",
   "metadata": {},
   "source": [
    "## **Maximum Likelihood Estimation (MLE)**\n",
    "- **MLE** is a method in statistics used to determine the values of a model's parameters that are most likely to have produced the observed data.\n",
    "- For instance, if you were estimating the average height of a population based on a sample, MLE helps identify the average height that maximizes the likelihood of observing the sample data.\n",
    "\n",
    "#### **General Likelihood Function**\n",
    "Let the observed data be $X = (x_1, x_2, \\dots, x_n)$, where $x_i$ represents individual data points, and let the unknown parameter(s) be denoted as $\\theta$.\n",
    "- The **likelihood function** $L(\\theta | X)$ represents the probability of observing the data $X$ given the parameters $\\theta$. It is calculated as:\n",
    "  $$ L(\\theta | X) = P(X | \\theta) = \\prod_{i=1}^{n} f(x_i | \\theta) $$ \n",
    "  - Here, $f(x_i | \\theta)$ is the probability (or probability density) of observing $x_i$ given the parameter $\\theta$.\n",
    "  - $\\prod_{i=1}^{n}$ indicates the product over all the data points $x_1, x_2, \\dots, x_n$.\n",
    "- The **log-likelihood function** is often used for simplification, as working with the logarithm of the likelihood can make calculations easier and more stable. The log-likelihood is given by:\n",
    "  $$ \\ell(\\theta | X) = \\log L(\\theta | X) = \\sum_{i=1}^{n} \\log f(x_i | \\theta) $$\n",
    "- To find the **Maximum Likelihood Estimate (MLE)** of the parameter(s), we maximize the log-likelihood function with respect to $\\theta$. The MLE is the value of $\\theta$ that maximizes this function:\n",
    "  $$ \\hat{\\theta} = \\arg \\max_{\\theta} \\ell(\\theta | X) $$\n",
    "    - Where $\\hat{\\theta}$ is the MLE of $\\theta$, which is the value that maximizes the likelihood of observing the given data.\n",
    "\n",
    "#### **Binomial Likelihood Function**\n",
    "As a specific example, consider the case where the data follows a binomial distribution. The binomial likelihood function expresses the probability of observing $s$ successes in $n$ trials, given a success probability $\\theta$. The likelihood is:\n",
    "  $$ L(\\theta) = \\binom{n}{s} \\cdot \\theta^s \\cdot (1 - \\theta)^{n - s} $$\n",
    "- Here:\n",
    "    - $\\binom{n}{s}$ is the binomial coefficient, representing the number of ways to choose $s$ successes from $n$ trials.\n",
    "    - $\\theta^s$ is the probability of $s$ successes.\n",
    "    - $(1 - \\theta)^{n - s}$ is the probability of $n - s$ failures.\n",
    "- The **log-likelihood function** for the binomial case is:\n",
    "  $$ \\ell(\\theta) = \\log L(\\theta) = \\log \\binom{n}{s} + s \\log \\theta + (n - s) \\log (1 - \\theta) $$\n",
    "- To find the MLE, we maximize this log-likelihood function with respect to $\\theta$.\n",
    "\n",
    "#### **Summary**\n",
    "Both the general likelihood function and the binomial likelihood function are part of the MLE framework. The general formulation applies to any statistical model, while the binomial likelihood function is specific to data following a binomial distribution.\n",
    "\n",
    "## Regression Analysis\n",
    "\n",
    "- The idea of **regression** is to fit a line that best describes the trend between two variables, minimizing the discrepancy between the model and the data. This line is defined as:\n",
    "    - $Y = ax + b$\n",
    "        - $a$ is the **slope** (regression coefficient)\n",
    "        - $b$ is the **y-intercept**\n",
    "        - $x$ is the **independent variable**\n",
    "        - $y$ is the **dependent variable**\n",
    "- We minimize the error (discrepancy) by measuring the differences between the line and the actual values. This error is calculated as:\n",
    "    - $\\text{SSR} = \\sum (\\hat{y}_i - y_i)^2$\n",
    "        - where $\\hat{y}_i = ax_i + b$ is the predicted value.\n",
    "        - SSR stands for **Sum of Squared Residuals**, representing the sum of squared differences between the observed ($y_i$) and predicted ($\\hat{y}_i$) values.\n",
    "\n",
    "### Regression Coefficient and Intercept:\n",
    "\n",
    "- The **regression coefficient** is a key concept in regression analysis. It represents the relationship between a predictor (independent variable $x$) and the outcome (dependent variable $y$). It indicates the strength and direction of the relationship between $x$ and $y$ in regression models.\n",
    "    - The regression coefficient ($a$) is given by the formula:\n",
    "        $$a = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}$$\n",
    "    - The **intercept** ($b$) is given by the average deviation:\n",
    "        $$b = \\bar{y} - a \\bar{x}$$\n",
    "        - where $\\bar{y} = \\frac{1}{N} \\sum_{i=1}^{n} y_i$ and $\\bar{x} = \\frac{1}{N} \\sum_{i=1}^{n} x_i$.\n",
    "\n",
    "### Correlation Coefficients:\n",
    "\n",
    "- **Correlation Coefficients** measure the degree of co-movement or association between two variables. It shows how strongly two variables are related to each other.\n",
    "    - The correlation coefficient ($r$) lies between -1 and +1:\n",
    "        - $r = 1$ indicates a strong positive correlation.\n",
    "        - $r = -1$ indicates a strong negative correlation.\n",
    "        - $r = 0$ indicates no correlation.\n",
    "    - To compute the correlation coefficient:\n",
    "        - For each pair of data points $(x_i, y_i)$, where $i = (1, \\dots, N)$:\n",
    "        $$r = \\frac{\\sum_{i=1}^N (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^N (x_i - \\bar{x})^2} \\cdot \\sqrt{\\sum_{i=1}^N (y_i - \\bar{y})^2}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fa699c-cd10-45fc-aeb5-1db0d3131f4e",
   "metadata": {},
   "source": [
    "## General Machine Learning Concepts\n",
    "\n",
    "- **Generalisation theory**, in simple terms, is about understanding how well a machine learning model can apply what it has learned from the training data to new, unseen data.\n",
    "- **Deterministic** models assume that there is no randomness in the system, so the same inputs will always return the exact same outputs.\n",
    "- **Stochastic** models assume that there is some randomness (noise) that we cannot account for in the system, meaning that the same input will give a slightly different output each time. This is exactly how real-world data works.\n",
    "- The **bias-variance tradeoff** is the balance, in machine learning, between building models that work well with both the data they have seen and new, unseen data. In simple terms:\n",
    "    - **High bias** means the model is too simple and misses important patterns in the data. This is called **underfitting**.\n",
    "    - **High variance** means the model is too complex and overly focused on the training data, capturing noise instead of true patterns. This is called **overfitting**.\n",
    "\n",
    "## Probabilistic Framework in Machine Learning\n",
    "\n",
    "- **Probabilistic Setting** refers to approaching machine learning problems from a probability theory perspective. Rather than making deterministic predictions, the model expresses uncertainty through probability distributions.\n",
    "- **Stationarity** is a property where the statistical characteristics of data remain constant over time. There are different types:\n",
    "    - ***Strict stationarity*** means the entire probability distribution stays unchanged over time.\n",
    "    - ***Weak stationarity*** (or covariance stationarity) means key statistical properties, such as mean and variance, remain constant over time.\n",
    "- **A Priori Knowledge** refers to information we know before analyzing the data—our assumptions and domain expertise that we build into the model. For example, physical constraints (like knowing values must be positive) are often used as prior knowledge.\n",
    "\n",
    "## Interpreting the Generalisation Bound\n",
    "\n",
    "- **Generalisation Bound** allows for the rigorous quantification of how much data is needed to make predictions about unseen data. It tells us how many samples $n$ are needed to be sure we select the correct function with a certain level of confidence. The generalisation bound is given by:\n",
    "    $$n \\geq \\frac{\\log\\left(\\frac{\\delta}{N-1}\\right)}{\\log(1 - \\epsilon)}$$\n",
    "    - $n$ = number of samples needed\n",
    "    - $\\delta$ = desired confidence level (in this case, 99.9% confidence, so $\\delta = 0.001$)\n",
    "    - $N$ = number of candidate functions (in this case, $N = 1000$)\n",
    "    - $\\epsilon$ = error rate of incorrect functions (in this case, 1%, so $\\epsilon = 0.01$)\n",
    "\n",
    "## Laplace’s Rule of Succession\n",
    "\n",
    "- **Laplace’s Rule of Succession** is a way of estimating the probability of an event happening in the future, based on past observations, while accounting for uncertainty with limited data:\n",
    "    $$P(\\text{next event is a success}) = \\frac{s+1}{n+2}$$\n",
    "    - $s$ is the number of successes observed (i.e., the event has happened $s$ times)\n",
    "    - $n$ is the total number of trials (i.e., the event has been observed $n$ times in total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10851e6e-1f30-4a90-bac3-f94259126b40",
   "metadata": {},
   "source": [
    "## Evaluting Model Performance\n",
    "\n",
    "### 1. Regression Models:\n",
    "##### **Mean Absolute Error (MAE)**  \n",
    "- MAE measures the average magnitude of errors between actual values ($y$) and predicted values ($\\hat{y}$).\n",
    "- A low MAE indicates that the model's predictions are close to the actual values on average.\n",
    "- A high MAE suggests larger average errors, meaning the model is less accurate.\n",
    "- $\\text{MAE = np.mean(np.abs(y - y1))}$\n",
    "$$ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$\n",
    "\n",
    "##### **Average Error (AE)**  \n",
    "- AE gives the average signed error, where positive and negative errors can cancel out.\n",
    "- A positive AE indicates overprediction on average.\n",
    "- A negative AE indicates underprediction on average.\n",
    "- $\\text{AE = np.mean(y - y1)}$\n",
    "$$ \\text{AE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i) $$\n",
    "\n",
    "##### **Mean Absolute Percentage Error (MAPE)**    \n",
    "- MAPE measures the average percentage error relative to the actual values.\n",
    "- A low MAPE suggests the model predicts well on a relative scale.\n",
    "- A high MAPE means that errors are large relative to the actual values.\n",
    "- $\\text{MAPE = np.mean(np.abs((y - y1) / y)) * 100}$\n",
    "$$ \\text{MAPE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| \\times 100 $$\n",
    "\n",
    "##### **Root Mean Squared Error (RMSE)**  \n",
    "- RMSE measures the square root of the average squared errors.\n",
    "- Penalizes large errors more heavily than MAE because of the squaring.\n",
    "- A low RMSE indicates better predictive performance.\n",
    "- A high RMSE highlights larger deviations in predictions.\n",
    "- $\\text{RMSE = np.sqrt(np.mean((y - y1) ** 2))}$\n",
    "$$ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $$\n",
    "\n",
    "##### **Total Sum of Squared Errors (SSE)**  \n",
    "- SSE measures the total squared deviation of predictions from actual values.\n",
    "- It represents the overall error without normalization.\n",
    "- A low SSE means better predictive performance, but its scale depends on the dataset size.\n",
    "- $\\text{SSE = np.sum((y - y1) ** 2)}$\n",
    "$$ \\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "### 2. Classification Models:\n",
    "#### Confusion Matrix\n",
    "\n",
    "A confusion matrix is a table used to evaluate the performance of a classification model. It compares the actual target values with the predictions made by the model. Here's an example of a confusion matrix layout:\n",
    "\n",
    "| Actual \\ Predicted | Positive | Negative |\n",
    "|---------------------|----------|----------|\n",
    "| **Positive**        | True Positive (TP)  | False Negative (FN) |\n",
    "| **Negative**        | False Positive (FP) | True Negative (TN)  |\n",
    "\n",
    "##### Explanation:\n",
    "- **True Positive (TP)**: The model correctly predicted the positive class.\n",
    "- **True Negative (TN)**: The model correctly predicted the negative class.\n",
    "- **False Positive (FP)**: The model incorrectly predicted the positive class when it was actually negative (Type I Error).\n",
    "- **False Negative (FN)**: The model incorrectly predicted the negative class when it was actually positive (Type II Error).\n",
    "\n",
    "##### Metrics Derived from the Confusion Matrix:\n",
    "1. **Accuracy**:  \n",
    "   The accuracy metric tells you the overall correctness of the model by calculating the proportion of correct predictions to the total predictions.  \n",
    "   $$\n",
    "   \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "   $$\n",
    "\n",
    "2. **Precision**:  \n",
    "   Precision measures the proportion of positive predictions that are actually correct. A higher precision means that when the model predicts positive, it is more likely to be correct.  \n",
    "   $$\n",
    "   \\text{Precision} = \\frac{TP}{TP + FP}\n",
    "   $$\n",
    "\n",
    "3. **Recall (Sensitivity)**:  \n",
    "   Recall (also known as sensitivity) measures the proportion of actual positives that are correctly identified by the model. A higher recall indicates that the model is better at identifying the positive class.\n",
    "   - focuses on the model's ability to correctly predict positives.\n",
    "   $$\n",
    "   \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "   $$\n",
    "\n",
    "5. **F1-Score**:  \n",
    "   The F1-score is the harmonic mean of precision and recall. It provides a balance between precision and recall and is particularly useful when the class distribution is imbalanced. A higher F1-score indicates a better balance between precision and recall.  \n",
    "   $$\n",
    "   \\text{F1-Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "6. **Specificity**:\n",
    "   Specificity measures the proportion of actual negatives that are correctly identified as negative by the model.\n",
    "   - Specificity focuses on the model's ability to correctly predict negatives.\n",
    "   $$\n",
    "   \\text{Specificity} = \\frac{TN}{FP + TN}\n",
    "   $$\n",
    "\n",
    "   \n",
    "8. **Estimation Misclassification Rate**\n",
    "    Shows the proportion of incorrect predictions\n",
    "    $$\n",
    "    \\text{Total Error Rate} = \\frac{FN + FP}{TP + FN + FP + TN}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5422d-2ee5-4c9a-ad58-1920d0d673e4",
   "metadata": {},
   "source": [
    "## Lift Charts \n",
    "- Lift:\n",
    "    - $\\text{Lift} = \\frac{\\text{True Positives in model-selected subset}}{\\text{Expected True Positives by random selection}}$\n",
    "    - Measures how much better a model performs compared to random selection.\n",
    "- Interpretation:\n",
    "    - A lift chart plots the cumulative percentage of actual positives (y-axis) against the cumulative percentage of samples (x-axis), ranked by predicted probability.\n",
    "    - The baseline (random model) follows a diagonal line, where a model with no predictive power achieves a lift of 1.\n",
    "    - A perfect model captures all positive cases early, creating a steep curve.\n",
    "- Use Cases:\n",
    "    - Commonly used in marketing, fraud detection, and customer targeting to rank predictions and prioritize high-value actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea10687f-6c7e-468f-b583-778e66c70681",
   "metadata": {},
   "source": [
    "## Techniques for fine-tuning the machine learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473c1e4a-1360-40a5-9d13-7b6539bad6c1",
   "metadata": {},
   "source": [
    "#### 1. K-Folds Cross Validation:\n",
    "- The dataset is split into $K$ equal-sized folds. The model is trained on $K-1$ folds and tested on the remaining fold. This process repeats $K$ times, each time using a different fold for testing.\n",
    "- **Cross-Validation Estimate**:\n",
    "    - $\\text{CV Error} = \\frac{1}{K} \\sum_{i=1}^{K} \\text{Error}_i$\n",
    "    - Computes the average error across all $K$ iterations.\n",
    "- Interpretation:\n",
    "    - Helps assess model performance by ensuring every data point is used for both training and testing.\n",
    "    - Reduces bias and variance compared to a single train-test split.\n",
    "- Common Variants:\n",
    "    - **Stratified K-Folds**: Maintains class distribution across folds (useful for imbalanced datasets).\n",
    "    - **Leave-One-Out Cross Validation (LOO-CV)**: Special case where $K$ equals the number of samples ($n$).\n",
    "- Use Cases:\n",
    "    - Model selection and hyperparameter tuning to ensure robust evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88fec43-c567-430c-a220-f79f3195ec30",
   "metadata": {},
   "source": [
    "#### 2. Oversampling\n",
    "- A technique used to address class imbalance by increasing the representation of the minority class to match the majority class.\n",
    "- Approaches:\n",
    "    - **Basic Duplication**: Creates identical copies of existing minority class samples.\n",
    "    - Synthetic Data Generation:\n",
    "        - **SMOTE (Synthetic Minority Oversampling Technique)**: Generates new samples by interpolating between existing minority class samples.\n",
    "        - **ADASYN (Adaptive Synthetic Sampling)**: Enhances SMOTE by generating synthetic samples in underrepresented regions.\n",
    "        - **Variational Autoencoders (VAEs) & GANs**: Create realistic synthetic data points, often used in image and text applications.\n",
    "        - **Weighted Oversampling**: Assigns higher weights to minority class samples instead of duplicating them.\n",
    "- Challenges:\n",
    "    - Overfitting: Repeating samples can lead to models memorizing instead of generalizing.\n",
    "    - Computational Cost: Increases dataset size, leading to higher training time.\n",
    "    - Synthetic Sample Quality: Poorly generated samples can negatively impact model performance.\n",
    "- **Stratified Sampling**\n",
    "    - Ensures that each class maintains its original proportion in both the training and test sets, preventing biased performance evaluation.\n",
    "    - Approach:\n",
    "        - Divide data into two groups:\n",
    "        - Set A: Minority class samples.\n",
    "        - Set B: Majority class samples.\n",
    "        - Construct training and validation sets:\n",
    "    - Training: Randomly select 50% from Set A and an equal number from Set B.\n",
    "    - Validation: Use the remaining 50% of Set A and add samples from Set B to restore the original class ratio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BaseGPU",
   "language": "python",
   "name": "basegpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
