{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8773db3-b998-42f3-a392-553ff64c4e2f",
   "metadata": {},
   "source": [
    "# Module 4 - Transformers\n",
    "\n",
    "## Contents\n",
    "- Part 1:\n",
    "    - 1.1 From Recurrance (RNN) to Attention-based NLP models\n",
    "    - 1.2 Transformer models, results and trade offs\n",
    "\n",
    "## Misc and Keywords\n",
    "- **Attention**: A mechanism that lets models weigh the importance of different parts of the input when making predictions.  \n",
    "- **Self-Attention**: A form of attention where queries, keys, and values all come from the same input sequence. Enables tokens to attend to each other.\n",
    "- **Positional Encoding**: Extra information added to token embeddings to provide a sense of order, since self-attention has no inherent sequence awareness.\n",
    "- **Multi-Head Attention**: Runs several self-attention operations in parallel on different representation subspaces and concatenates the outputs.\n",
    "- **Causal Masking**: A technique to prevent tokens from attending to future positions in a sequence, used in language generation tasks.\n",
    "- **Residual Connections (Add & Norm)**: A pattern where the input to a layer is added to its output, followed by layer normalisation. Helps with gradient flow and training stability.\n",
    "- **Feed-Forward Networks**: Position-wise fully connected layers applied after self-attention, usually with non-linear activations like ReLU or GELU.\n",
    "- **Encoder**: A component of the transformer that processes the entire input sequence at once, typically using unmasked attention.\n",
    "- **Decoder**: A transformer component that generates output sequences one token at a time, using masked attention to prevent future peeking.\n",
    "- **Transformer**: An architecture built entirely on attention mechanisms, forgoing recurrence and convolution, enabling efficient parallel training.\n",
    "- **Autoregressive Generation**: A decoding method where the model predicts one token at a time, feeding each prediction back into the model.\n",
    "- **Parallelisation**: The ability to compute multiple operations simultaneously — a key advantage of transformers over RNNs.\n",
    "\n",
    "### Notes:\n",
    "- Self-attention enables all-pair interactions, unlike RNNs which are limited by sequential processing.\n",
    "- Transformers achieve state-of-the-art results in many NLP tasks, but at the cost of high computational demand.\n",
    "- Encoding position is essential in transformers since token order is not inherently preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be878169-4219-4df8-b726-c888f197e837",
   "metadata": {},
   "source": [
    "# Part 1: Transformers\n",
    "\n",
    "## 1.1 From Recurrence (RNN) to Attention-based NLP Models\n",
    "\n",
    "### Issues with Recurrent Models (RNNs)\n",
    "\n",
    "- **Linear Interaction Distance**\n",
    "    - RNNs process sequences sequentially (unrolled left to right).\n",
    "    - This enforces **linear locality**—i.e., nearby words influence each other more than distant ones.\n",
    "    - Makes learning **long-range dependencies** difficult due to vanishing gradients and time-step bottlenecks.\n",
    "\n",
    "- **Lack of Parallelisability**\n",
    "    - RNNs require sequential computation:\n",
    "        - Forward and backward passes are inherently **non-parallelisable**.\n",
    "    - This limits training speed and scalability.\n",
    "\n",
    "### Enter Attention\n",
    "\n",
    "- **Attention Mechanism**\n",
    "    - Each word's representation acts as a **query** that interacts with a set of **keys and values** (often from other words).\n",
    "    - This allows direct modelling of **relationships between all tokens**, regardless of distance.\n",
    "    - It solves:\n",
    "        - **Long-distance dependency modelling** (by allowing all-pair interactions).\n",
    "        - **Parallel computation** (since all token interactions can be computed simultaneously).\n",
    "\n",
    "### Self-Attention\n",
    "\n",
    "- In **Self-Attention**, the **queries**, **keys**, and **values** all come from the **same sequence**.\n",
    "- The mechanism computes:\n",
    "\n",
    "  $$\n",
    "  \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "  $$\n",
    "\n",
    "  Where:\n",
    "    - $Q$, $K$, $V$ are the query, key, and value matrices.\n",
    "    - $d_k$ is the dimensionality of the keys (used for scaling).\n",
    "    - Each token attends to every other token, capturing context-rich representations.\n",
    "\n",
    "### Barriers to Using Self-Attention\n",
    "\n",
    "These barriers must be addressed in order to construct a minimal self-attention building block.\n",
    "\n",
    "#### No Implicit Sequence Order\n",
    "- Unlike RNNs, self-attention does not inherently encode the order of tokens in a sequence.\n",
    "- To address this, **positional encodings** are added to token embeddings.\n",
    "\n",
    "**Types of Positional Encodings:**\n",
    "- **Sinusoidal**: Use fixed sine and cosine functions with varying frequencies to encode position.\n",
    "- **Learned**: Introduce trainable position embeddings that are optimised during training.\n",
    "\n",
    "#### No Nonlinearities for Deep Learning 'Magic'\n",
    "- Self-attention on its own is a linear operation.\n",
    "- To introduce nonlinearity and model more complex functions, a **position-wise feedforward neural network** is applied to the output of the self-attention layer.\n",
    "- The same feedforward network is applied independently to each position.\n",
    "\n",
    "#### Need to Prevent Looking into the Future\n",
    "- In tasks like language modelling, it is important to **prevent tokens from attending to future tokens**.\n",
    "- This is achieved using **causal masking**, which ensures that each position can only attend to earlier positions (or itself). Future words are 'masked'\n",
    "\n",
    "## The Self-Attention Building Block\n",
    "\n",
    "The self-attention mechanism is used as a core component in transformer architectures. Below is the standard processing flow for generating output probabilities from input tokens:\n",
    "\n",
    "**Processing Steps:**\n",
    "\n",
    "1. **Inputs** – Raw token sequence (e.g., words or subwords).\n",
    "2. **Embeddings** – Convert each token into a dense vector representation using an embedding layer.\n",
    "3. **Add Positional Embeddings** – Inject information about token order using either sinusoidal or learned positional encodings.\n",
    "4. **Masked Self-Attention** – Compute self-attention while preventing each token from attending to future tokens (used in autoregressive tasks).\n",
    "5. **Feed-Forward Network** – Apply a position-wise feedforward neural network with nonlinearities to each token's representation.\n",
    "6. **Linear Layer** – Project the outputs into vocabulary-size logits for prediction.\n",
    "7. **Softmax** – Convert logits into probabilities.\n",
    "8. **Output Probabilities** – Final distribution over the next possible tokens.\n",
    "\n",
    "This block forms the basis of each decoder layer in the transformer architecture.\n",
    "\n",
    "## 1.2 Transformer Models, Results and Trade-Offs\n",
    "\n",
    "### Transformer Decoder\n",
    "\n",
    "The decoder is the core of many language models, such as GPT. It is designed to generate text one token at a time, conditioning on previously generated tokens.\n",
    "\n",
    "**Processing Steps:**\n",
    "\n",
    "1. **Embeddings**\n",
    "    - Converts input tokens into dense vector representations (word embeddings).\n",
    "\n",
    "2. **Add Positional Embeddings**\n",
    "    - Injects information about the position of each token, since self-attention alone does not encode sequence order.\n",
    "    - These positional embeddings are added element-wise to the token embeddings.\n",
    "\n",
    "3. **Masked Multi-Head Attention**\n",
    "    - Allows each token to attend to previous tokens in the sequence (and itself), but not future tokens.\n",
    "    - Multi-head attention allows the model to jointly attend to information from different representation subspaces.\n",
    "\n",
    "4. **Add & Norm**\n",
    "    - Residual connection: the input to the attention layer is added to its output.\n",
    "    - Layer normalisation is applied to stabilise training.\n",
    "\n",
    "5. **Feed-Forward Network**\n",
    "    - A fully connected network applied independently to each position.\n",
    "    - Usually consists of two linear layers with a ReLU or GELU nonlinearity in between.\n",
    "\n",
    "6. **Add & Norm**\n",
    "    - Another residual connection: the input to the feed-forward network is added to its output.\n",
    "    - Followed by layer normalisation.\n",
    "\n",
    "The final output can then be passed through a linear projection and softmax to obtain token probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "### Transformer Encoder\n",
    "\n",
    "The encoder is typically used in models for understanding tasks (e.g. BERT), where the full input sequence is available at once.\n",
    "\n",
    "**Key Differences:**\n",
    "- Uses **unmasked multi-head attention** — each token can attend to all others in the sequence.\n",
    "- The rest of the structure mirrors the decoder, but without the masking step.\n",
    "\n",
    "Encoder and decoder can be combined (as in the original Transformer architecture) for tasks like translation.\n",
    "\n",
    "---\n",
    "\n",
    "### Results and Trade-Offs\n",
    "\n",
    "**Advantages:**\n",
    "- Highly parallelisable, enabling fast training.\n",
    "- Excellent performance on a wide range of NLP tasks.\n",
    "- Long-range dependencies can be modelled directly.\n",
    "\n",
    "**Trade-Offs:**\n",
    "- Self-attention has quadratic complexity with respect to sequence length ($O(n^2)$), limiting scalability for very long texts.\n",
    "- Requires large amounts of data and compute to train effectively.\n",
    "- Positional encoding is a workaround rather than an inherent solution for sequential structure.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BaseGPU",
   "language": "python",
   "name": "basegpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
